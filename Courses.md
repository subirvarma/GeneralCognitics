---
layout: default
title: "Courses"
---

I teach courses on Deep Learning and Reinforcement Learning in the Information Systems and Analytics Department, at the Leavey School of
Business at Santa Clara University.

**Deep Learning Course**

Here are the slides from my lectures for the Deep Learning course

1. [Lecture 1](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture1_Introduction.pdf) - **Introduction:** Introduction to Deep Learning and discussion of important applications, Introduction to Types of Deep Learning Systems: Supervised Learning, Reinforcement Learning, Unsupervised Learning, Self-Supervised Learning, An historical overview of Deep Learning

2. [Lecture 2](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture2_MathematicalPreliminaries.pdf) - **Mathematical Preliminaries:** An overview of Probability Theory, Bayes Rule, Random Variables, Random Sequences, Markov Chains, Maximum Likelihood Estimation, Basics of Linear Algebra, Matrices, Tensors

3. [Lecture 3](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture3_LinearNetworks.pdf) - **Linear Networks:** The Classification and Regression Problems, Solving these using Supervised Learning, Binary Classification, Linear Models (Logistic Regression), Loss Functions, Introducing Gradient Descent, K-ary Classification, Using Keras to solve linear models

4. [Lecture 4](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture4_DFN.pdf) - **Dense Feed Forward Networks:** Interpreting the Linear Classifier, Dense Feedforward Networks, The Backprop Algorithm, Forward and Backward Passes, Gradient Flow Algebra, Derivation of the Backprop Algorithm, Dense Feedforward Networks using Keras

6. [Lecture 5](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture5_Backprop.pdf) - **Backprop:** Gradient Flow Calculus, Forward and Backward Passes in Backprop, Derivation of Backprop

7. [Lecture 6](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture6_Keras.pdf) - **Keras:** Training Process, Some Common Training Datasets: MNIST, CIFAR-10, ILSVRC, IMDB etc, Getting Deeper into Keras, Ingesting Data into Keras Models: Image, Text and Tabular

8. [Lecture 7](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture7_Training1.pdf) - **Training Part 1:** Vanishing Gradient Problem, Activation and Loss Functions, Techniques to Improve Stochastic Gradient Descent, Illustration of algorithms using Keras, Instructions for doing Term Project

9. [Lecture 8](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture8_Training2.pdf) - **Training Part 2:** Weight Initialization, Data Pre-Processing, Batch Normalization, Model Under-fitting and Over-fitting problems, Illustration of Algorithms using Keras

10. [Lecture 9](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture9_Training3.pdf) - **Training Part 3:** Regularization Techniques – L2, L1 and Dropout, Dataset Augmentation, Hyper-Parameter Selection – Manual and Automated Tuning, Model Ensembles, Illustration of Algorithms using Keras

11. [Lecture 10](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture10_CNN1.pdf) - **Convolutional Networks Part 1:** History and Applications of ConvNets, ConvNet Architecture, 2D Convolutions, 1D Convolutions, Sizing ConvNets, Modeling ConvNets with Keras

13. [Lecture 11](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture11_CNN2.pdf) - **Convolutional Networks Part 2:** Pooling and Padding in ConvNets,  Trends in ConvNet Design: Small Filters, Global Max Pooling, Depthwise Separable Convolutions, Some Historically significant ConvNet Architectures – LeNet5, AlexNet, ZFNet, VGGNet 

14. [Lecture 12](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture12_CNN3.pdf) - **Convolutional Networks Part 3:** ConvNet Architectures (cont): InceptionNet, ResNet. DenseNet, SqueezeNet, Transfer Learning using Keras, Text and Tabular Data Processing using 1D Convolutions

15. [Lecture 13](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture13_RNN1.pdf) - **Recurrent Networks Part 1:** RNN Architectures – One to One, Many to One, Many to Many; Contrasting RNNs with ConvNets, Deep and Bi-Directional RNNs, Combination of RNNs and ConvNets

16. [Lecture 14](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture14_RNN2.pdf) - **Recurrent Networks Part 2:** Difficulties in Training RNNs and how to solve them, Back Propagation through Time (BPTT) Algorithm, LSTMs, GRUs, Word Embeddings and the Word2Vec algorithm, Modeling RNNs with Keras

17. [Lecture 15](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture15_NLP1.pdf) - **Natural Language Processing Part 1:** Application of RNNs to Natural Language Processing, Probabilistic Language Models, Beam Search, Softmax Temperature, Text Classification, Machine Translation, Attention Mechanism in RNNs

18. [Lecture 16](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture16_NLP2.pdf) - **Natural Language Processing Part 2:** Image Captioning, Question Answering Systems, Reading Comprehension, Information Retrieval Systems, Speech Transcription

19. [Lecture 17](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture17_Transformers1.pdf) - **Transformers Part 1:** Self Attention, Transformer Architecture, Multiple Attention Heads, Encoding Position Information, Visualizing Attention Patterns

20. [Lecture 18](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture18_Transformers2.pdf) - **Transformers Part 2:** Language Models using Transformers, Encoders Decoders Using Transformers, BERT Bi-Directional Language Models, Image Processing Using Transformers

21. [Lecture 19](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture19_DiffusionModels.pdf) - **Diffusion Models:** Latent Variables, ELBO Bound, Forward and Backward Diffusion Processes, DDPM and DDIM Algorithms

22. [Lecture 20](https://subirvarma.github.io/GeneralCognitics/Course1/Lecture20_ReinforcementLearning.pdf) - **Reinforcement Learning** Introduction, Components of a RL System: Agents, Rewards, Actions, Deep RL, Playing Pong with Policy Gradients, Playing Go with Supervised Learning and Policy Gradients, Imitation Learning

