# What Would Kant Think of LLMs?

## Introduction

In the the late 1700s, the great German Philosopher Immanuel Kant, revolutionized his field through the work *Critique of Pure Reason*. This was probably the most significant advance in philosophy since the time of the Greeks, and influenced the course of the subject since the time it was published. Around the same time modern science was also making great strides. It emerged in the 17th century with luminaries such as Galileo and Newton, but charted its own course idependent of philosophy. In part this was due to the separation between science and the mind that was put into place by the great French thinker Descartes, at the dawn of the Scientific Age. The separation finally started break down in the 20th century when it became clear that the new science of Quantum Mechanics had some deep problems if we try to interpret it within the existing scientific framework which didn't take the mind into account. 

In our century we have another emerging science (or is Technology?) of Artificial Neural Networks (ANNs) and the attendent subfield of Large Language Models (LLMs). The latter serve as models for human language and there are open questions whether these models are actually modeling human cognition.
A deeper understanding of a proper framework for science that takes both the mind as well as ANNs into account has become important due to these developments.  Getting a better understanding of how mathematical models, whether in Physics or in Artificial Intelligence, relate to reality and our perceptions of reality is critical in resolving these questions. 

Fortunately there are several new ideas that scientists and philosophers have come up with in the last two decades and it seems that some of Kant's ideas are starting to become very relevant and we have finally started to make some progress with  problems ranging from the interpretation of Quantum Mechanics to theories of how the mind works. In this essay I will start by surveying the state of knowledge in these fields, followed by some ideas on how these can be applied to the interpretation of mathematical models for language in the form of LLMs.

Here is a brief definition of the main actors in this drama:

- Base Reality: This is reality as it exists independent of human perception.
- Perceived Reality: Information from Base Reality impinges on our senses, and our brain puts it all together to create the world as we see, hear and feel it.
- Mathematics and Mathematical Models: Mathematics can be defined as a self-consistent system of thought, operating using the rules of logic. It can be used to create models of reality, as was discovered by Galileo and Newton more than 300 years ago.
- The Mind: Our brain consists of a large number of interconnected neurons, which is somehow able to generate the mind and all the subjective experiences that go along with it.
- Artificial Neural Networks: These are also mathematical models, but of a special kind since they serve as models for the brain. These models are very different than the classical Newtonian models that are used in Physics.

The rest of this essay is organized as follows: In Section 2 we discuss the Penrose framework for Scientific Knowledge, which proposes a relationship between Perceived Reality, Human Conition and Mathematical Models. Section 3 is on the relationship between Base Reality and Perceived Reality as discussed by Kant and others, Section 4 is on Mathematical Models of Reality and our current understanding of good these models are in describing it. We propose a modification to Penrose's framework in Section 5, and in Section 6 we add LLMs to the mix and discuss how it might fit within this modified framework.

## The Kantian Framework

Immanuel Kant set out for himself the task to discovering the limits of human knowledge and the result was his book "Critique of Pure Reason", that was published almost 250 years ago. At the time of Kant, there was a debate in philosophical circles about the nature of our perception of the world. The philosophical school of Idealism, whose most prominent proponent was Bishop Berkeley (but also had predecessors such as the Neo-Platonic School and Ancient Indian Philosophy) claimed that all perception is entirely a creation of the mind, and reality as such does not exist outside the mind. The other school  was that of Realism which said that there is in fact an objective reality that exists independent of our minds, and what we see around us is like a window into that reality. Kant's innovation was to propose a middle way between these philosophies, which he called Transcedental Idealism. According to it, our perception of the world incorporates elements from two sources: 

- Source 1 is the information coming to us from the external world to our sensory organs. Hence there is an external world that does exist out there, unlike what the Idealists claimed, but we don't have access to it using our senses (hence the Transcedentalism). Kant called this the Noumenon.
- Source 2 comes from our minds, which processes the information from Source 1 and creates the world as we perceive it, called the Phenomenon. However this picture of the world does not reflect the Noumenon or base reality, but is a creation of our mind, with the objective of helping us survive.

Fundamental to our perception of the world are the concepts of Space (or the fact that objects have an extension in space) and Time (or the fact that events have a duration in time). Kant claimed that Space and Time are purely concepts created by our brain (or *a priori* concepts) to bring order to the chaos of information coming to our senses, and do not exist in the Noumenon. Our perceptions are not things in the world, they are versions of those things that we construct in our minds by shaping them in space and time. 
Hence what we perceive is the world that our brain creates using the information from the Noumenon coming through our senses. However this information is very different than the world that we actually perceive, for example when we see a red object in front of us, the sensation of redness is not part of the light photons coming from that object.

Kant did not argue for the existence of the Noumenon using empirical experiments, but instead relied on rhetoric and philosopical arguments. His main arguments are known as Kant's Antinomies which are logical contradictions that arise when we confuse the phenomenal world and the noumenal world. Since the time of Kant the idea of the Noumenon has been much debated. Philosophers such as Hegel and Nietzsche have objected to the way Kant separated the world in two layers, and they said that since the Noumenon is effectively unknowable, it is irrelevant to practical philosophy. Other philosophers such as Schopehauer accepted this idea and tried to develop it further.
  
(https://subirvarma.github.io/GeneralCognitics/images/Kant3.jpg) 

The book " The Rigor of Angels: Borges, Heisenberg, Kant and the Ultimate Nature of Reality" has an excellent description of the main elements of Kant's philosophy, and also has a fascinating discussion of the similarities between it, the short stories of the Argentinian writer Jorge Luis Borges and basic ideas behind Quantum Mechanics as formulated by Werner Heisenberg. The following discussion of Kant's ideas borrows heavily from the presentation in this book:

For sensory input to become knowledge of the world requires that objects be located in respect to other things and that events be sequenced as coming before, after or simultaneous to other events. But locating objects with respect to one another in space, or sequencing events in time, is something that pure sense perception on its own cannot accomplish. This exposure needs to be organized in order to experience it.
In order to experience the spatial relations between objects we implicitly assume that they exist together in a shared space. However space itself is not an object of experience, but the very condition for experiencing objects. Space (and time) are like representations that our brain creates so that we can experience reality.
Consciousness is another entity creates by our minds in order to create a timeline that unifies our experiences and orders our perceptions.

We make a fundamental error by assuming that Space and Time are objects in themselves that exist in Base Reality, just like other objects that we see around us. But instead Space and Time are created by our brains as a way to mediate between the chaotic amount of data that is constantly impinging on our senses, by organizing them within this space-time framework or representation. Several classical paradoxes, such as that of Zeno, arise because of our tendency to treat Space-Time as a real object, followed by our application of the mathematical idea of infinite divisibility, to conclude that space-time can be divided into smaller and smaller segments infinitely many times. However the idea of infinite divisibility is also not part of the Phenomenon, it exists purely in the world of Mathematical concepts. The Laws of Physics are really the laws of our observations of how things behave in our Perceived Reality. There is a very nice quote by Heisenberg in this regard: "We have to remember that what we observe is not nature in itself but nature exposed to our method of questioning".

A nice metaphor for Kant's ideas of Space-Time is the way in which we use Cartesian co-ordinates to represent objects in geometry. Clearly the co-ordinate system is not another object, but instead it is an abstraction used to organize the objects that exist in mathematical space. Morever the co-ordinate system is not unique, in the sense that we can choose some other system, such as Polar Co-ordinates if we think that it supplies a better representation.

Once we accept the idea that Space-Time is not part of the Noumenon, but instead are constructs created by our brains to organize sensory data, then the question arises, what exactly is in the Noumenon? To this Kant's response is: We don't know, and we can never know. If we were to exist in the Noumenon, then we would have the ability to operate outside the limitations of Space and Time. However all of our knowledge comes from operating within the limitations of time and space. Operating outside it would mean seeing everything simultaneously or knowing all time in an instant, which would obliterate the very connection between objects and instances that constitutes knowledge. Since an observation is always an observation in time and space, the Laws of Physics are all about operations in time and space. We may speculate about the world outside the conditions of time and space, but nontemporal and nonspatial perspectives obliterate the very idea of an observation, and hence are incompatible with any knowledge we can have of the world.

Kant's ideas came before the discovery of Quantum Mechanics and Relativity, and so a natural question is how well they have held up in light of these theories. The short answer is that that they have held up quite well. Heisenberg was inspired by the ideas of Kant in creating his Matrix Mechanics. He derived his equations with the purpose of explaining the world as it appears to our senses (and scientific instruments), and did not to try to visualize a human comprehensible model that lies behind those equations, a decidedly Kantian approach. Indeed he later come up with the idea of the Uncertainity Principle that showed that our ideas of position and momentum do not extend to the microscopic world. 
Electrons and other microscopic entities are not objects in the sense that we see around us, and it is impossible for us to visualize what they are, since they appear either as particle or a wave depending upon the experiment we perform. 
This sounds a little bit like Kant's ideas of the Noumenon, since that too contains objects that we can't visualize. However there is difference between the two, namely that the microscopic objects still exist within the framework of space-time and their properties can be accessed using scientific instruments and hence cannot be part of the Noumenon. They are still part of the Phenomemon, but these entities are so outside the realm of our daily experience that our language does not have the words to describe them and the only way to talk about them is by using the language of mathematics.
Einstein himslf never reconciled to the idea that the models in Physics were not describing our objective reality, hence is oft quoted remark that "God does not play dice with the universe" when he was confronted by the indeterminism at the base of Quantum Theory. 

The other domain of science that might shed light on the existence of the Noumenon is Cognitive Science, which is the study of how the neurons in our brain process information. Later in this blog we will describe one of the theories in this area by the neuro-scientist Anil Seth that was proposed recently. This theory has a decidedly Kantian flavor to it, and tries to account for visual perception (and lots of other functions of the mind) using ideas whose origin can be traced back to Kant.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant15.png) 

Figure 1

Fig. 1 summarizes Kant's views: It shows signals such as light and sound waves coming from the Noumenon which impinge on our senses. These are then processed by the brain, which abstracts the deluge of data, and presents a picture of reality to us which is tailored so as to help us survive in our environment. Since all data coming from the Noumenon is filtered through the brain, we cannot get direct access to the Noumenon.

## Philosophical Frameworks for Science

Kant's ideas were developed about a hundred years after the start of modern science with Newton and Galileo. Science initially developed and made progress using a different framework than the one proposed by Kant, called Cartesian Dualism. This framework achieved great success and gave us the modern world. However as described later in the section, it has run into problems in recent times, which have led people to re-examine the basic ideas behind the framework.

Galileo and Newton made a monumental discovery more than 400 years ago, which is that the Laws of Nature lend themselves to a mathematical description. This adds another actor to our drama, namely the space of Mathematical Systems. 
What is a Mathematical System? These are systems that contain numerical or geometric objects, that are defined solely by the way they interact with other objects. The interaction is in terms of mathematical operations (that differ from system to system). The work of mathematicians consists of deriving interesting properties of the system using the rules of logic. 
At one time it was thought that Mathematical Systems reflect our reality, in the sense that the objects and rules in the system were abstractions of objects and rules in the the real world. However this prejudice  was exploded in the 19th century when mathematicians came up with Non-Eucledian Geometries and Number Systems that have nothing to do with our reality, but were studied purely because they resulted in interesting mathematical results. Some of these systems later found applications in modeling nature, much to the surprise of scientists. 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant8.png) 

Figure 2

Our tale begins with Rene Descartes, the founder of Co-ordinate Geometry, which was one of his many achievements. Descartes also came up with the idea of Cartesian Dualism as the philosophical foundation for science, which is illustrated in Fig. 2. His big idea was to banish the mind from within the domain of science so that explanations for the mind and its subjective sensations were placed outside the scope of science. Instead science was to limit itself to modeling the objective aspects of Nature which could be measured in a precise manner using instruments. 
Descartes was motivated by several objectives in making this proposal. One big motivation was probably to keep the mind outside the scientific purview and make it part of the religous world, which was probably wise given what had happened to Bruno and Galileo around the same time that Descartes lived. Another motivation was to accelerate the progress of science by limiting it to things which could be measured in an objective fashion. 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant13.png) 

Figure 3

The Cartesian Framework for science has undergone a few changes since the seventeenth century, especially after the Age of Enlightenment in the 1700s, it was no longer taboo to include the mind as part of the framework. The generally accepted modern framework for science is well described by the eminent physicist Roger Penrose in his book "The Road to Reality", and this is shown in Fig. 3. This framework (which we will refer to as the post-Cartesian framework) states that:

- There is another entity called the Physical World which has its own objective reality independent of the Mental World. Our perception is like a window on to the Physical World and reflects a true picture of what exists in Nature.
- There is a world of mathematical objects, that is a creation of the human mind and a subset of this world serves as a model for the Physical World. A stronger version of this belief is that the Physical World at its core is nothing else but a set of mathematical equations (for an example of this see the book "Our Mathematical Universe" by Max Tegmark).
- Our brain is one of the objects that exists in the Physical World. Since other aspects of the Physical World can be modeled using mathematics, it should be possible to come up with a model for the brain which should be able to explain aspects of the mind.

The post-Cartesian framework was highly successful in discovering laws that govern the physical world. After the the initial development of Newtonian Dynamics, other aspects of nature such Electromagnetic Waves and the Laws of Thermodynamics were discovered in the 19th century, and they all fell within this framework.
The first crack in the framework happened in the beginning years of the 20th century, with the discovery of energy quanta by Max Plack. Within a few years of that, Einstein proposed that light exhibits both wave and particle properties, and indeed within a couple of decades experiments were conducted that showed that material particles like electrons also have wavelike properties such frequency. This raised a conundrum since for the first time the fundamental constituents of nature had a form that could not be visualized by us humans. Soon after this Heisenberg followed by Schrodinger proposed a mathematical description of microscopic phenomena that was unlike any theory that had come before. Schrodinger's version of the theory was based on a fundamental quantity called the wave function (or $\psi(x,t)$) whose nature was completely mysterious. After a lot of debate, Max Born proposed that the square of the wave function gave the probability distributon for the quantum particle being to be found in certain volume of space, which meant that the dynamics of microscopic particles  was fundamentally random. 

In parallel several interpretations of Quantum Mechanics were proposed, the most prominent being what is called the Copenhagen Interpretation. This does not try to explain the wave-particle duality, but simply states that the particle can exhibit either depending upon the experiment being performed. A quantum particle that is not being observed can be in a superposition of several states at the same time, and does not settle into a definite state until it is observed. This leads to the paradox of Schrodinger's Cat being dead and alive at the same time (this has been tested experimentally with the Aspect experiments which resulted in the 2022 Nobel Prize in Physics, not with a cat though :-)). The Copenhagen Interpretation also states that the wave function is an objective (i.e., observer independent) measure of the uncertainity that we have about the Quantum World, and this uncertainity vanishes for objects in the macro world, in other words Quantum Mechanics comes into effect for microscopic objects but is not relevant at normal scales. This last claim of the Copenhagen Interpretation has been experimentally shown to be false, so that even macro objects can exhibit wave particle duality.

The Copenhagen Interpretation is a way to shove the interpretive problems in Quantum Theory under the rug, and proceed with the day to day applications of the theory and that has worked work well in practice. However one can think of experiments in which the Copenhagen Interpretation fails, and the most well known of these is known as the problem of Wigner's Friend, which is as follows: Assume that there is a closed box containing a quantum particle that can be in one of two states, and and an Observer 1 who is located outside the box. In addition the system containing the box and Observer 1 is itself in a larger box which is closed to the outside. There is also a second observer, say Observer 2 (Wigner's friend) who is located outside the larger box. Lets say Observer 1 at some point opens the box and finds that the partcle is in State 1, at this point the Wave Function from Observer 1's point of view has 'collapsed' since there is no more uncertainity about the state of the partcle. However Observer 2 does not know this, and from his point of view, the combined system consisting of the particle and Observer 1 can be one of several possible quantum states. From this experiment it follows that there is no single observer independent interpretation of the wave function, i.e., the wave function is an a subjective measure of uncertainity in a quantum system, and cannot be separated from the observer. This is precisely the Quantum Bayesianism or Qbism interpretation of Quantum Mechanics that was proposed recently. According to Qbism the wave function is a subjective measure of quantum uncertainity and exists only in the mind of the observer, and is not really part of physical reality as such. This strikes at the heart of the post-Cartesian Framework since we can no longer claim that science is discovering Laws of Nature that exist independent of our mind. Hence the QBism interpretation of Quantum Mechanics clearly does not fall within the traditional scientific framework shown in Fig. 3, and opens the door to examining Kant's ideas as an alternative, which is discussed in the next section.
A good description of these topics can be found in Chapter 5 of the book "Putting Ourselves Back in the Equation" by George Musser.

Other problems with the post-Cartesian framework for science emerge when we try to use it to explain the mind.
The framework actually opens up the possibility of modeling the brain using mathematical models since the framework says that the brain is part of the physical worldand since the mind arises from the brain, we should be able to model itusing mathematics. 
This raises the question: What is the mathematicalmodel for the mind?  How can a set of physical processes happening within the brain give rise to our mind and its subjective experiences? 

And therein lies the great mystery: 
How does the mind reflect the " objective physical reality" into the image of the world that we see in front of us?
This has no explanation within the post-Cartesian Framework, and is referred to as the "Hard Problem of Consciousness". 
In order to explain the mind, perhaps we need to go beyond the post-Cartesian Framework, just as Qbism did for Quantum Mechanics. In the next section we discuss in detail whether the Kantian Framework can solve the problems that are inherent in the post-Cartesian Framework.

## A Kantian Framework for Science

In the previous section we described how the equations of Quantum Mechanics don't describe an objective observer independent reality, but instead provide predictions that are tied to the mind of the observer doing the experiment. This appears very much like the picture of the world that Immanuel Kant drew in his philosophy. In this section we pursue this line of thinking, and also investigate how the Kantian Framework also applies to models of the mind.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant9.png) 

Figure 4

The Kantian view of Science is illustrated in Figure 4. There is no longer a split between the Physical World and the Mental World as in Figure 3, since our perceptions of reality are a creation of our minds and hence are part of the Mental World. The mathematical theories that we come up with are not describing the Noumenon, but are instead modeling the subjective perceptions created by our minds. This might also explain why mathematics is so effective in modeling nature, this is because both mathematics and nature (as we experience it) are creations of our minds. 
The Kantian framework says that just as perceived reality is a creation of mind, at the same time the mind is one of the objects that exists in this reality. This circular path is ofen referred to as a "Strange Loop" in Cognitive Science.

An interesting aspect of this framework is that mathematical models of nature are no longer considered to be a reflection of some deep seated objective reality that exists independent of our perception. If so, what are they? Why do they seem to reflect the operation of the world?
A way to interpret these models is by using an idea called Instrumentalism, which basically says that mathematical models are a way to capture the regularities in our observations, nothing more than that. The main proponent of this point of view was the Austrian phycist Ernst Mach, who was a big influence on both the early Einstein and Heisenberg. In the process of deriving the equations the scientist may hypothize some underlying mechanism, such as fields to explain electromagnetism. But these fields are a mathematical convenience and do not reflect any underlying reality. In the next section we will see that mathematical models for the mind carry this way of thinking further, such that the models are essentially a black box connecting inputs to desired outputs. For a deeper dive into this idea, please see my blog post ["Latent Variables and Latent Spaces"](https://subirvarma.github.io/GeneralCognitics/2023/12/20/LatentVariables.html).

The QBism interpretation of Quantum Mechanics clearly falls within the Kantian Framework, since both say that the practice of science, at the most fundamental level, is a purely subjective endeavour. But what about theories of the mind?
There have been proposals for theories of the mind works which operate within the Kantian framework. The German scientist Herman von Helmholtz proposed one such theory in the late nineteenth century according to which the contents of perception are not given by sensory signals themselves but have to be inferred by the brain by combining these signals with the brain's expectations or beliefs about their causes.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant16.png) 

Figure 5

More recently the British neuroscientist Anil Seth has extended Helmholtz's ideas by making use what we have learnt about the brain's operation since then, which is nicely summarized in his book "Being You: The New Science of Consciousness". His ideas are best captured by the notion of perception as a *controlled hallicunation* (see Fig. 5) and are summarized as follows:

- The brain's circuitry incorporates generic models for objects that we encounter in our lives. When the circuit is activated, the brain projects an image of the object onto our visual field.
- The brain is also constantly making predictions about the causes of its sensory signals, and these cascade in a top-down manner through the brain's perceptual hierarchies. These signals stream into the brain bottom-up (or from the outside) and keep these perceptual predictions tied  to the objects from which they originated. These signals serve as prediction errors registering the difference between what the brain expects and what it gets at every level of processing. By adjusting the top-down predictions so as to suppress the bottom-up prediction errors, the brain's perceptual best guesses maintain the connection with what is happening in the world. Thus perception happens through a continual process of prediction error minimization. The use of Bayesian ideas here as well as in the QBism interpretaion of Quantum Mechanics is interesting.
- Perceptual experience is determined by the content of the top-down predictions and not by the bottom-up sensory signals. Thus we never experience the bottom-up sensory signals themselves, but only the top-down interpretations of them.
- The brain does its predictions using the well known Baye's Rule from probability theory, with the current prediction serving as the prior, the likelihoods encode mappings from potential candidate objects that might have generated the sensory signals. The resulting posterior serves as what actually gets perceived, and also serves as the prior for the next round of predictions.
- If the perceptual priors are very strong, then it results in an actual hallicunation. On the other hand when we are paying closer attention to the external world, then the sensory signals dominate, and this results in a picture that is closer to reality.

Anil Seth's model for visual perception can be considered to be a realization of the idea that Kant originally proposed in the "Critique of Pure Reason": A way in which the chaos of the noumenal world are organized by the mind and projected into a picture of reality that we see round us.
In his book Seth also talks about how other aspects of the mind, such the perception of change, time, moods, emotions and even the perception of the self, can also be explained by his predictive + generative model. He breaks up the hard problem of modeling the mind, into smaller problems of individual aspects of the mind, which can be tied to particular patterns of brain activity. 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant21.png) 

Figure 6

Based on Seth's work, I have modified the Kantian framework as shown in Fig. 6. I have broken up perceived reality into two types:

- Phenomenon A is what is used in scientific theories. Scientists typically choose a few 'objective' quantities from their observations, such as the mass, velocity, electric charge etc which go into their mathematical models. The quantities so chosen depend on the nature of the scientific theory being proposed.
- Phenomenon B is of the type that Seth talks about, i.e., the reality that we see when we open our eyes. The mind uses its own intrinsic models to generate this reality for us. Note that Phenomenon A is a subset of Phenomenon B, and does not capture the richness of our visual perception.

The main pupose for the PDE models of reality that we find in physics, is to make predictions about the future state of the system that is being modeled. Interestingly, Anil Seth's theory of visual perception is also based on the principle that the brain generates our reality by making predictions. But there is a fundamental difference between the two. The predictions of visual reality made by our brain take place automatically, and we are not even aware that it is doing so. This ability is also presumably something that we share with other animals. On the other hand the predictions made by physics are an uniquely human endeavor and require our capability for logic and mathematics. We are also capable of making longer term predictions that are outside of physics such as: How long will it take me to drive to work ttoday based on the traffic state, or what kind of salary can I expect if I get an advanced degree etc. These point to a capability of our brains to generate future scenarios and do planning based on that, and may be something that is unique to us.

The Scientific Revolution gave us mathematical models for Phenomenon A. Similarly is it possible to come up mathematical models for Phenomenon B? In other words can we mimic the minds ability to generate images and other aspects of reality using generative models that are also based on mathematics? This is explored in the rest of this essay.

There is a philosophical school called Phenomenoligism that arose in the 20th century, founded by the philosopher Husserl and his student Heidegger. They sought to debunk the idea that by doing physics we are plumbing the depths of reality, since the equations of physics are two levels removed from the base reality: Level 1 being the abstraction created  by our minds, and Level 2 being the further abstraction whereby certain aspects of our perceived reality are chosen to be part of models used in the equations of physics. Hence our subjective experiences are infinitely more complex than any physical theory can capture, and we can never hope to describe all of it using mathematics. For an accessible overview of recent developments in this philosophy see the book "The Blind Spot: Why Science Cannot Ignore Human Experience" by Frank, Gleiser and Thompson.

## Mathematical Models for the Mind

In the previous section we described a Kantian Framework for science that proposes that since the brain is one of the objects that exists in our perceived reality, we should be to model it using mathematics, and the mind should arise from such a model. The mind is special case, since we use the mind  for creating  reality, and hence in a sense the mind is trying to model itself within the confines of the perceptual boundaries that it has created. In the prior section we described Anil Seth's "Controlled Hallicunation" theory of the mind which provides mechanisms by which specific aspects of the mind emerge, and it also operates within the Kantian Framework. In this section and the in the succeeding two, we will look at how ANN models exhibit properties that are similar to certain aspects of the the mind, in particular we will look at mathematical models for visual perception and language.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant11.png) 

Figure 7

Fig. 7 proposes that mathematical models for the brain take the form of Artificial Neural Networks (ANNs). These models are different than Partial Differential Equations (PDEs) that are used in Physics, and point to a fundamentally different way for building mathematical models of reality. 

- Models for scientific observations (on the LHS of Fig. 7): These take the form of PDEs and are created by scientists. In order to do so they focus on certain aspects of reality that can be captured using numbers. The process by which they are able to "guess" the right equations remains a mystery of human creativity, but some of the things they rely on includes prior work by other scientists, experimental results, mathematical theories and lastly and most importantly their own intuition.
Even though PDE models for physical reality make use of advanced mathematics, at their root they are simple, in the sense that they are able to model complex physical phenomena using only a few equations with tens of parameters. It can be argued that the reason why this is even possible is because of the *a priori* structure that our minds create in order to organize the chaos of data coming from the Noumenon. According to Kant these structures include our perceptions of space and time, and these simplify these models of quite a bit, since these imply that all interaction between entities in the model is limited to entities that are in the (space-time) neigborhood of each other. Without this simplifying assumption building mathematical models of physical reality would be impossible.

- Models for visual perception (on the RHS of Fig. 7): These take the form of ANNs that can have billions of neurons interacting with each other. Unlike our mind, ANNs don't come with any simplifying *a priori* structures like space and time which have been built up over hundreds of millions of years of evolution. Indeed ANNs incorporate models for individual neurons that can potentially interact with all other neurons in the model, which makes these models extremely complex. Consequently we also need billions of parameters to model all these interactions, so these models are very different than the parameter sparse PDE models. It is impossible to obtain these parameters using experimentation, and instead they are estimated using a process called training. During training, we try to match the input into the model with its correct output, and if they don't match then we modify the model parameters slightly using an algorithm called Stochastic Gradient Descent. This process id repeated hundreds of millions of times until the model is fully trained. There is no logical justification for the resulting parameter values other than the fact that they lead to 'correct' outputs given an input, so the model is not comprehensible in the sense of models in physics, it is a Black Box.
Before training starts, the "mind" of the ANN can be considered to be a *Tabula Rasa" or a blank slate since all their parameters are set to random values (strictly speaking we do endow the ANN with an pre-existing connection topology, so it is not entirely a *Tabula Rasa*). Which is why it takes so much a data to train the ANN, in some sense we are trying to reproduce the gradual process of natural evolution in biological brains within a few days of training. During this time the ANN has to discover the structure of the world through the images and text that we feed into it.

In some sense the images used as part of the training data can be considered the be the equivalent of the Nuomenon for ANNs. Just as our mind organizes the chaos of the actual Noumenon using structures as space and time, similarly ANNs must use some organizing principle to structure the chaos of pixel data in their training sets. As a result of the training, the ANN parameter values are no longer random, but assume values that enable it to recognize patterns that are present in the data.
When a pre-trained ANN is subject to new data that is not part of the training set, then this is analogous to sensory data coming into our brain. Due to its training, the ANN is now able to extract meaning from the data and carry out tasks that are useful to us. 

What are these organizing principles that an ANN uses? Are they similar to the concept of Space and Time that emerge in our minds? We will discuss this question in the following sections.

** What about planning? **

## ANN Models for Visual Perception

Just as the brain organizes the chaotic light and sound data coming from the Noumenon into our orderly perception of the world, ANN models for vision are able to organize the pixels, which are just blobs of color, into images that make sense to us. How do they do this?

Anil Seth's work is an example of a theory that explains visual perception in our minds. It involves a number of different mechanisms, the main ones being the ability to generate images of objects from information that is already in our minds and the ability to predict the next 'frame' of the visual perception. In this section we will focus on one of them, and see how ANNs can be used to mimic the mind's ability to generate images.
This property of ANNs was discovered in the early days of Deep Learning, when ANN models were mostly used for image classification which was done by feeding image pixels into a model, with the output of the model corresponding to the probability distribution over the various image categories. Researchers found out that when this model was run backwards, i.e., the output was stimulated with the category corresponding to a particuler image category, then the pixels coming out at the other end actually looked like the object coresponding to that category. It didn't look exactly like the images that were fed into the ANN during the training process, but one could see that they were derived from them.
When I was first learning about ANNs, I remember being fascinated by this property, and it fed my subsequent interest in these systems. The fact that the ANN was able to reproduce images meant that it was not merely learning enough information from the training data to classify images, but it was learning higher level information about the object that was sufficient for it to be able to reproduce its likeness.
Since those very early examples of image generation, the state of the art has progressed quite a bit, with the latest models such as Open AI'sDALLE-3 or Google's Gemini being able to generate photo realistic images.

![](https://subirvarma.github.io/GeneralCognitics/images/lat51.png) 

Figure 8

The simplest way to understand how ANNs generate images is to think of it as a mapping from the space of image pixels, to a higher level representation called a Latent Vector (which is just a bunch of numbers organized in one or two dimensions). A Latent Vector captures the information in the image, but in a highly compressed form. Images have a lot structure in them, which leads to redundancies in the way they are expressed using pixels. The ANN strips away all these redundancies and is able to capture the patterns in the image using just a few numbers, and these are summarized in the form of a Latent Vector. These vectors exist in a higher dimensional space called Latent Space, and every image maps to one of the vectors in this space. The Latent Space is a fantastically complex mathematical object that exists in a space with hundreds of dimensions, to which we had no access to before the advent of ANNs. It has the nice property that if we interpolate between the Latent Vectors for two images, then all the vectors lying on the linear segment connecting the two also correspond to valid images (such an object is called a manifold in Differential Geometry). Hence the ANN maps image pixels to points in the image manifold, and it generates images by reversing this operation, as explained next.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant17.png) 

Figure 9

The general principle for training an ANN to generate images is shown in Fig. 9, and you may notice that it is quite similar to the Anil Seth's model for visual perception. There are two operations Encoding and Decoding.
The Encoding operation (shown in the RHS path in Fig. 9) can be likened to processing the chaotic information coming in the form of pixels, and this is done in an hierarchical fashion, which ultimately results in the creation of a Latent Vector corresponding to the image. 
The Decoding process involves turning a Latent Vector back into pixels, as is shown in the path on the LHS of Fig. 9. 
This involves taking the bare bones representation of the image in the Latent Vector form and gradually adding the shapes and textures into it until it becomes a fully formed image. Different ANN models use different techniques to implement the Encoding and Decoding pipelines, and currently the best system that we have are called Diffusion Models.

The brain is a much more complex system than ANNs, given that each neuron can have 1000s of dendrites connecting it to other neurons, and it is not fully understood how it is able to perform the encoding and decoding operations. We do have some idea of how the brain does encoding, and this was discovered in the 1960s in a series of famous experiments carried by by David Hubel and Torsten Wiesel. In fact the ANN called Convolutional Neural Network was subsequently designed to mimic what we had learnt about the brain from these experiments. The generation process in the brain is much more complex than simply inverting a Latent Vector, and involves inputs from a large number of brain regions.
The neurons in our brains do a much better job of modeling the external world, given that we are able to learn from just a few examples if new objects come into our field of vision, whereas much more data is required for training an ANN. It is quite likely that the ability to this is one of the *a priori* capabilities that has been built in our brains over 100s of millions of years of biological evolution. 

We mostly talked about the generation aspect of ANNs in this section, but what about the other ingredient in Seth's theory, i.e., the ability to Predict the next image frame? ANNs can also do image prediction, as shown by their ability to generate video clips. However this technology is still being perfected, and they are currently limited to generating short clips. Also somethimes the generated video betrays ignorance of basic physics, such as a glass object bouncing when dropped on the floor instead of shattering. The ANN based Diffusion Models used for image generation can be extended to generate video clips, with the difference being that multiple image frames (corresponding to the video) are now generated in one shot in the Decoder pipeline shown in Fig. 9. More fundamentally, videos are also captured and reduced to a Latent Vector representation, just as for images.

## Language within the Kantian Framework

So now we finally come to the topic that was advertised in the title of the essay, namely human language. Just like visual perception, mathematics, science and ANNs, language is a creation of the human mind. It is a very old creation, it is estimated that human language arose about 50,000 years ago, and it also co-incided with a leap in human cognition that also happened around the same time. Philosophers did not turn their attention towards language until the 20th century, and the name that stands out is that Ludwig Wittgenstein. He came up with a theory of language in his work *Tractatus Logico-Philosophicus* which was published in 1918.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant20.png) 

Figure 10

The main aspects of Wittgenstein's theory of language are summarized below (and also shown in Fig. 10): 
- Both language and the world have structure
- Language consists of propositions, which are compounds of 'elementary propositions', which in turn are combinations of 'names. Names are the ultimate constituents of language.
- The world consists of the totality of facts, which are compounded out of 'states of affairs', which in turn are combinations of 'objects'.
- Each level of structure in the world is matched by a level of structure in language: Names denote objects, combinations of names constitute elementary propositions that correspond to state of affairs, and each of these in their turn combine to form, respectively propositions and facts.
- The arrangement of names at the most fundamental level of language structure 'mirrors' or 'pictures' the arrangement of objects at the most fundamental level of the world's structure. This called the 'picture theory of meaning' and is central to the philosophy.
- Thoughts are what can be expressed using language and a proposition in a language only true if it can translated into a picture. Only propositions in science satisfy this requirement. The propositions in mathematics (which is also a type of language) are tautologies, i.e., they are always true regardless of the state of the world.
- For propositions in religion, philosophy, ethics, aesthetics etc, they cannot be expressed in words (that satisfy the criterea). For these 'showing'rather than 'saying' is all that is possible. They are not pictures of of actual or possible facts, and therefore are meaningless. This however does not mean that they are un-important, Wittgenstein though that they are are the truly important parts of human knowledge, its just that we cannot talk about them to figure out whether they are true or false.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant23.png) 

Figure 11

Fig. 11 shows a way in which we can fit language within the Kantian Framework. It captures the fact that language is another creation of the brain, and also Wittgenstein's ideas of the relationship between language and visual perception, as well as the idea that language also captures the propositions of science. Hence in some sense language is the more powerful creation of the mind, since other aspects of human generated reality can be described using it. 

## ANN Models for Language

![](https://subirvarma.github.io/GeneralCognitics/images/Kant18.png) 

Figure 12

ANN models for language proceed along the same lines as those for vision, with the main difference being that image pixels are now replaced by individual words (there are LLMs that can be built by directly using characters instead, and those work quite well too). As shown in the top part of Fig. 12, LLMs map pieces of text to points in the language Latent Space. 
Just as for images, language has a lot of structure in the relationship between words, and the ANN is able to capture this structure and therby reduce the representation of the piece of text into a highly compressed form with just a few numbers. As in images, these Latent Vectors live in a high dimensional Latent Space, such that each point in this Latent Space corresponds to a piece of text. The discovery of the language Latent Space seems to have settled a long standing open problem in Artificial Intelligence of how to generate language which sounds like it came from a human (also known as the Turing Test). 

There are two aspects of langauge, namely syntax and semantics. Syntax denotes the rules which capture the grammer in language while semantics captures the meaning. There were logical models for syntax before the advent of ANNs, but nobody had been able to find a mathematical model that captures the semantics. The Latent Space for language seems to be one such model, and given the complexity of the mathematical structure within which it exists, it explains why it hadn't been discovered before the advent of ANNs. Interestingly enough the same ANN model, namely Transformers, has been used to build models for both image and language, which points to an underlying similarity in the structure that underlies them. We explore this further in the following section.

The bottom part of Fig. 12 shows the process by which LLMs are trained. The text from the training dataset is fed into the LLM, which then converts it into a vector in Latent Space. This vector is then decoded to regenerate the original text, and the difference between the reproduced text and the original text is used as an error signal to train the model. In practice the text is input into the model on a word by word basis, and the output text is also produced likewise word by word. LLMs are trained using huge datasets that encompasses most of the language that can be found in books and online.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant22.png) 

Figure 13

Fig. 13 shows the LLM as integrated within the Kantian framework. It shows that both LLMs and Image ANNs serve as mathematical mappings from the language and image Latent Spaces to actual language and images respectively.

Language is an all purpose technology that we humans have invented that enable us to communicate and as Wittgenstein pointed, it also serves as a model for the world. Moreover the language of science (and mathematics) is encompassed within this. This raises the interesting question: Since LLMs serve as a mathematical model for language, do LLMs inherit language's ability to generate all of human perception and scientific knowledge. We will get back to this question later in this essay, but before that lets examine the connection between language and images as revealed to us by ANNs.


## A Wittgenstian Connection Between Images and Text Generated by ANNs

![](https://subirvarma.github.io/GeneralCognitics/images/lat35.png) 

Figure 14

Wittgenstein hypothized that there is a one-to-one or isometric mapping between propositions in language and images. It turns out there is a similar isometry between text and images generated by LLMs, and this exists at the Latent Space level (see Fig. 14). 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant25.png) 

Figure 15

One can start with a Latent Vector in the text Latent Space, and convert it into a piece of text using an LLM. We can also take the Latent Vector in the image Latent Space that corresponds to the text Latent Vector and also convert it into image. We will find that the semantic content of both the generated text and the image are the same. Hence the isometry between text and images is lade bare once we look at their Latent Spaces.

![](https://subirvarma.github.io/GeneralCognitics/images/lat33.png) 

Figure 16

Fig. 16 shows the process by which a piece of text is converted into an image: We start with a piece of text, and use the LLM to convert it into its Latent Vector. We then take the Latent Vector and fed into an image generating ANN to get the corresponding image.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant26.png) 

Figure 17

Not all text that is generated by an LLM corresponds to the real world, the subset that does are called True Propositions by Wittgenstein. The subset of True Propositions maps one to one to the subset of images that correspond to our shared reality. Similarly there are subsets of the language Latent Space and the Image Latentr Space that correspond to True Propositions and they also map one to one to each other (this is shown in Fig. 17).. 

How can a LLM tell whether a vector in its text Latent Space translates into a True Proposition? It cannot, this is something only a human can do since only humans are in direct contact with reality (this is the well known problem of hallucinations in LLMs). The world that is described to the LLM during its training consists of text taken from the open Internet and from books, and some of these may not be true. If an LLM cannot differentiate between True and False Propositions, then how can it be used to do science? We will investigate this question a the following section.

## Can LLMs Do Mathematics or Science?

![](https://subirvarma.github.io/GeneralCognitics/images/agent9.png) 

Figure 18

There is a big difference between the kind of thinking our brain engages in when it is doing mathematics or science (which is the process shown on the LHS of Fig. 13), vs the kind of thinking involved in generating images or language. This is the same distinction that was made by the psychologists Kahnemann and Twersky, who referred to the latter as System 1 thinking and to the formar as System 2 thinking. System 1 thinking is intuitive, we are not even aware the work that the brain does to generate the reality we see, or come up with a grammatically and syntactically correct statement. System 2 thinking on the other hand involves conscious effort on our part, and is probably unique to humans. This is the kind of thinking we do when we are doing mathematics or science, or even doing planning in our daily lives (such as how to organize a vacation). 

System 2 thinking can be captured in the tree graph shown in Fig. 18, which is of the type used in a subfield of Artificial Intelligence called Reinforcement Learning (RL). There are two kinds of nodes in the graph: The white nodes correpond to States of the world, while the black nodes correspond to Actions that an Agent might take. Hence fundamental to RL is the idea of an Agent, which in this case is either or brain or another ANN. The States in the graph can be images, for example when we are driving a car, the Actions correspond to movements of the steering wheel or the brakes, while State corresponds to the changing view of the world that we see. Alternatively when we are trying to solve a math problem, the State is the current state of our calculation (assuming it involves several steps), while the Action corresponds to mathematical operations that act upon the State.

System 2 thinking can be accompanied either by going out and taking Actions in the real world to acheive some objective (for example drive to work), or alternatively it can be carried out entirely in our head. The latter mode of thinking is called Planning, and it can be done in two contexts:

- Solving a problem in mathematics: Solving a math problem does not involve any interaction with the world, and is entirely an abstract exercise. Wittgenstein pointed out that the Propositions of mathematics are tautologies, i.e., they are always True irrespective of the state of the world. Mathematics does not have anything to do with reality, mathematical systems are are built using logic, and as long as there are no errors in the logical process, the resulting propositions should be correct.

- Solving a problem that involves knowledge of the world: Consider the example of trying to assemble a piece of furniture using instructions that came with it. When we actually assemble the piece then of course that corresponds to taking Actions in the real world, but assume that before the assembly starts we are mentally figuring out te various steps involved. The latter is an example of Planning, and in order to carry it out we need a to create a mental picture of how the furniture would look if we carried out an assembly step which would then correspond to the State.

From the last example we can see that in order to plan out the task successfully, our mind needs to give us an accurate picture of the reality that would result if we were to carry out a particular Action. This is precisely the problem in using ANNs to do the Planning, since with the current state of the art, the ANN has no means of knowing whether the image it is generating corresponds to reality or not.

From this discussion we can reach the following conclusons:

- ANNs can be used to solve mathematical problems since it does not involve any interaction with the world. They can also be used to solve problems in which they are controlling Agents who are constantly receiving feedback about their Actions from the real world, as in the case of a robot or a self driving car.
- ANNs cannot be used to solve Planning problems that involve the generation of accurate models of the real world. 

Problems in physics or even the solution of Planning problems we encounter in our daily lives belong to the latter class of problems. 










-------------------------------------------------------------------------------------------------------------------------------

## Artificial Neural Networks and LLMs

- The ANN is a model for human cognition
- Extending Kant’s thinking: The ANN can never be the same as cognition
- Mistaking ANNs for cognition can lead to antinomies
- But ANNs can approximate cognition
- Perceived Reality —> Language model of Perceived Reality —> LLM model of language 
- There is some unknown factor that makes the biological brain conscious, ANN models are not close to discovering this
- Can Integrated Information Theory ( IIT) be used as a measure of how close an ANN is to human cognition 

## LLMs within a Kantian Framework

Three latent spaces
1. latent space of perception of reality, either images or language or both together 
2. Latent space of reality is physics
3. Latent spaces of mathematics 
4. Our brain

- latent space of models of reality (physics) can be mapped on to a subset of the latent space of mathematics
- Our brain creates a mapping from perceptions to the latent space of perceptions . The points of this latent space are stored in the brain. 
- Mathematical latent  space is a creation of our brain

There are two latent spaces of reality
- Space 1 is a mapping in the brain to individual points of our perception of reality 
- Space 2 is a mathematical mapping, that is useful for doing predictions (physics). This is a mapping from our perception of reality to latent spaces of physics. 
- The mathematical space used in 2 is also a creation of the brain
- Hence physics does the following: 1) identify mathematical objects that can map to our perceptions of reality, such as fields, groups etc 2) Come up with mathematical equations that work on the mathematical objects and can be used for doing predictions (involving time and space)
- Are the regularities that we observe in nature a creation of the human brain? Then it makes sense that another human creation, ie mathematics, can be used to describe those regularities
- These regularities exist in time and space, and according to Kant, the these are a creation of the brain

Another aspect of the Kantian Framework is that Mathematical Models are always approximations and not a true picture of reality at the fundamental level. This is due to the fact these models are two levels removed from the Base Reality or the Noumenon. At the top level our minds create an abstraction of the Base Reality, and the Mathematical Models create a second level of abstraction since they are modeling the Perceived Reality which is itself an abstraction. This also means that Mathematical Models are only as good as the predictions that they make and we should not mistake the model for reality.
