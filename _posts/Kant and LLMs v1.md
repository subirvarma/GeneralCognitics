# What Would Kant Think of LLMs?

## Introduction

In the the late 1700s, the great German Philosopher Immanuel Kant, revolutionized his field through the work *Critique of Pure Reason*. This was probably the most significant advance in philosophy since the time of the Greeks, and influenced the course of the subject since the time it was published. Around the same time modern science was also making great strides. It emerged in the 17th century with luminaries such as Galileo and Newton, but charted its own course idependent of philosophy. In part this was due to the separation between science and the mind that was put into place by the great French thinker Descartes, at the dawn of the Scientific Age. The separation finally started break down in the 20th century when it became clear that the new science of Quantum Mechanics had some deep problems if we try to interpret it within the existing scientific framework which didn't take the mind into account. 

In our century we have another emerging science (or is Technology?) of Artificial Neural Networks (ANNs) and the attendent subfield of Large Language Models (LLMs). The latter serve as models for human language and there are open questions whether these models are actually modeling human cognition.
A deeper understanding of a proper framework for science that takes both the mind as well as ANNs into account has become important due to these developments.  Getting a better understanding of how mathematical models, whether in Physics or in Artificial Intelligence, relate to reality and our perceptions of reality is critical in resolving these questions. 

Fortunately there are several new ideas that scientists and philosophers have come up with in the last two decades and it seems that some of Kant's ideas are starting to become very relevant and we have finally started to make some progress with  problems ranging from the interpretation of Quantum Mechanics to theories of how the mind works. In this essay I will start by surveying the state of knowledge in these fields, followed by some ideas on how these can be applied to the interpretation of mathematical models for language in the form of LLMs.

Here is a brief definition of the main actors in this drama:

- Base Reality: This is reality as it exists independent of human perception.
- Perceived Reality: Information from Base Reality impinges on our senses, and our brain puts it all together to create the world as we see, hear and feel it.
- Mathematics and Mathematical Models: Mathematics can be defined as a self-consistent system of thought, operating using the rules of logic. It can be used to create models of reality, as was discovered by Galileo and Newton more than 300 years ago.
- The Mind: Our brain consists of a large number of interconnected neurons, which is somehow able to generate the mind and all the subjective experiences that go along with it.
- Artificial Neural Networks: These are also mathematical models, but of a special kind since they serve as models for the brain. These models are very different than the classical Newtonian models that are used in Physics.

The rest of this essay is organized as follows: In Section 2 we discuss the Penrose framework for Scientific Knowledge, which proposes a relationship between Perceived Reality, Human Conition and Mathematical Models. Section 3 is on the relationship between Base Reality and Perceived Reality as discussed by Kant and others, Section 4 is on Mathematical Models of Reality and our current understanding of good these models are in describing it. We propose a modification to Penrose's framework in Section 5, and in Section 6 we add LLMs to the mix and discuss how it might fit within this modified framework.

## The Kantian Framework

Immanuel Kant set out for himself the task to discovering the limits of human knowledge and the result was his book "Critique of Pure Reason", that was published almost 250 years ago. At the time of Kant, there was a debate in philosophical circles about the nature of our perception of the world. The School of Idealism, whose most prominent proponent was Bishop Berkeley (but also had predecessors such as the Neo-Platonic School and Ancient Indian Philosophy) claimed that all perception is entirely a creation of the mind, and reality as such does not exist outside the mind. The other school  was that of Realism which said that there is in fact an objective reality that exists independent of our minds, and what we see around us is like a window into that reality. Kant's innovation was to propose a middle way between these philosophies, which he called Transcedental Idealism. According to it, our perception of the world incorporates elements from two sources: 

- Source 1 is the information coming to us from the external world to our sensory organs. Hence there is an external world that does exist out there, unlike what the Idealists claimed, but we don't have access to it using our senses (hence the Transcedentalism). Kant called this the Noumenon.
- Source 2 comes from our minds, which processes the information from Source 1 and creates the world as we perceive it, called the Phenomenon. However this picture of the world does not reflect the Noumenon or base reality, but is a creation of our mind, with the objective of helping us survive.

Fundamental to our perception of the world are the concepts of Space (or the fact that objects have an extension in space) and Time (or the fact that events have a duration in time). Kant claimed that Space and Time are purely concepts created by our brain (or *a priori* concepts) to bring order to the chaos of information coming to our senses, and do not exist in the Noumenon. Our perceptions are not things in the world, they are versions of those things that we construct in our minds by shaping them in space and time. 
Hence what we perceive is the world that our brain creates using the information from the Noumenon coming through our senses. However this information is very different than the world that we actually perceive, for example when we see a red object in front of us, the sensation of redness is not part of the light photons coming from that object.

Kant did not argue for the existence of the Noumenon using empirical experiments, but instead relied on rhetoric and philosopical arguments. His main arguments are known as Kant's Antinomies which are logical contradictions that arise when we confuse the phenomenal world and the noumenal world. Since the time of Kant the idea of the Noumenon has been much debated. Philosophers such as Hegel and Nietzsche have objected to the way Kant separated the world in two layers, and they said that since the Noumenon is effectively unknowable, it is irrelevant to practical philosophy. Other philosophers such as Schopehauer accepted this idea and tried to develop it further.
  
(https://subirvarma.github.io/GeneralCognitics/images/Kant3.jpg) 

The book " The Rigor of Angels: Borges, Heisenberg, Kant and the Ultimate Nature of Reality" has an excellent description of the main elements of Kant's philosophy, and also has a fascinating discussion of the similarities between it, the short stories of the Argentinian writer Jorge Luis Borges and basic ideas behind Quantum Mechanics as formulated by Werner Heisenberg. The following discussion of Kant's ideas borrows heavily from the presentation in this book:

For sensory input to become knowledge of the world requires that objects be located in respect to other things and that events be sequenced as coming before, after or simultaneous to other events. But locating objects with respect to one another in space, or sequencing events in time, is something that pure sense perception on its own cannot accomplish. This exposure needs to be organized in order to experience it.
In order to experience the spatial relations between objects we implicitly assume that they exist together in a shared space. However space itself is not an object of experience, but the very condition for experiencing objects. Space (and time) are like representations that our brain creates so that we can experience reality.
Consciousness is another entity creates by our minds in order to create a timeline that unifies our experiences and orders our perceptions.

We make a fundamental error by assuming that Space and Time are objects in themselves that exist in Base Reality, just like other objects that we see around us. But instead Space and Time are created by our brains as a way to mediate between the chaotic amount of data that is constantly impinging on our senses, by organizing them within this space-time framework or representation. Several classical paradoxes, such as that of Zeno, arise because of our tendency to treat Space-Time as a real object, followed by our application of the mathematical idea of infinite divisibility, to conclude that space-time can be divided into smaller and smaller segments infinitely many times. However the idea of infinite divisibility is also not part of the Phenomenon, it exists purely in the world of Mathematical concepts. The Laws of Physics are really the laws of our observations of how things behave in our Perceived Reality. There is a very nice quote by Heisenberg in this regard: "We have to remember that what we observe is not nature in itself but nature exposed to our method of questioning".

A nice metaphor for Kant's ideas of Space-Time is the way in which we use Cartesian co-ordinates to represent objects in geometry. Clearly the co-ordinate system is not another object, but instead it is an abstraction used to organize the objects that exist in mathematical space. Morever the co-ordinate system is not unique, in the sense that we can choose some other system, such as Polar Co-ordinates if we think that it supplies a better representation.

Once we accept the idea that Space-Time is not part of the Noumenon, but instead are constructs created by our brains to organize sensory data, then the question arises, what exactly is in the Noumenon? To this Kant's response is: We don't know, and we can never know. If we were to exist in the Noumenon, then we would have the ability to operate outside the limitations of Space and Time. However all of our knowledge comes from operating within the limitations of time and space. Operating outside it would mean seeing everything simultaneously or knowing all time in an instant, which would obliterate the very connection between objects and instances that constitutes knowledge. Since an observation is always an observation in time and space, the Laws of Physics are all about operations in time and space. We may speculate about the world outside the conditions of time and space, but nontemporal and nonspatial perspectives obliterate the very idea of an observation, and hence are incompatible with any knowledge we can have of the world.

Kant's ideas came before the discovery of Quantum Mechanics and Relativity, and so a natural question is how well they have held up in light of these theories. The short answer is that that they have held up quite well.  
Heisenberg was inspired by the ideas of Kant in creating his Matrix Mechanics. He derived his equations with the purpose of explaining the world as it appears to our senses (and scientific instruments), and did not to try to visualize a human comprehensible model that lies behind those equations, a decidedly Kantian approach. Indeed he later come up with the idea of the Uncertainity Principle that showed that our ideas of position and momentum do not extend to the microscopic world. 
Eectrons and other microscopic entities are not objects in the sense that we see around us, and it is impossible for us to visualize what they are, since they appear either as particle or a wave depending upon the experiment we perform. 
This sounds a little bit like Kant's ideas of the Noumenon, since that too contains objects that we can't visualize. However there is difference between the two, namely that the microscopic objects still exist within the framework of space-time and their properties can be accessed using scientific instruments and hence cannot be part of the Noumenon. They are still part of the Phenomemon, but these entities are so outside the realm of our daily experience that our language does not have the words to describe them and the only way to talk about them is by using the language of mathematics.
Einstein himslf never reconciled to the idea that the models in Physics were not describing our objective reality, hence is oft quoted remark that "God does not play dice with the universe" when he was confronted by the indeterminism at the base of Quantum Theory. 

The other domain of science that might shed light on the existence of the Noumenon is Cognitive Science, which is the study of how the neurons in our brain process information. Later in this blog we will describe one of the theories in this area by the neuro-scientist Anil Seth that was proposed recently. This theory has a decidedly Kantian flavor to it, and tries to account for visual perception (and lots of other functions of the mind) using ideas whose origin can be traced back to Kant.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant15.png) 

Figure 1

Fig. 1 summarizes Kant's views: It shows signals such as light and sound waves coming from the Noumenon which impinge on our senses. These are then processed by the brain, which abstracts the deluge of data, and presents a picture of reality to us which is tailored so as to help us survive in our environment. Since all data coming from the Noumenon is filtered through the brain, we cannot get direct access to the Noumenon.

## Philosophical Frameworks for Science

Kant's ideas were developed about a hundred years after the start of modern science with Newton and Galileo. Science initially developed and made progress using a different framework than the one proposed by Kant, called Cartesian Dualism. This framework achieved great success and gave us the modern world. However as described later in the section, it has run into problems in recent times, which have led people to re-examine the basic ideas behind the framework.

Galileo and Newton made a monumental discovery more than 400 years ago, which is that the Laws of Nature lend themselves to a mathematical description. This adds another actor to our drama, namely the space of Mathematical Systems. 
What is a Mathematical System? These are systems that contain numerical or geometric objects, that are defined solely by the way they interact with other objects. The interaction is in terms of mathematical operations (that differ from system to system). The work of mathematicians consists of deriving interesting properties of the system using the rules of logic. 
At one time it was thought that Mathematical Systems reflect our reality, in the sense that the objects and rules in the system were abstractions of objects and rules in the the real world. However this prejudice  was exploded in the 19th century when mathematicians came up with Non-Eucledian Geometries and Number Systems that have nothing to do with our reality, but were studied purely because they resulted in interesting mathematical results. Some of these systems later found applications in modeling nature, much to the surprise of scientists. 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant8.png) 

Figure 2

Our tale begins with Rene Descartes, the founder of Co-ordinate Geometry, which was one of his many achievements. Descartes also came up with the idea of Cartesian Dualism as the philosophical foundation for science, which is illustrated in Fig. 2. His big idea was to banish the mind from within the domain of science so that explanations for the mind and its subjective sensations were placed outside the scope of science. Instead science was to limit itself to modeling the objective aspects of Nature which could be measured in a precise manner using instruments. 
Descartes was motivated by several objectives in making this proposal. One big motivation was probably to keep the mind outside the scientific purview and make it part of the religous world, which was probably wise given what had happened to Bruno and Galileo around the same time that Descartes lived. Another motivation was to accelerate the progress of science by limiting it to things which could be measured in an objective fashion. 

![](https://subirvarma.github.io/GeneralCognitics/images/Kant13.png) 

Figure 3

The Cartesian Framework for science has undergone a few changes since the seventeenth century, especially after the Age of Enlightenment in the 1700s, it was no longer taboo to include the mind as part of the framework. The generally accepted modern framework for science is well described by the eminent physicist Roger Penrose in his book "The Road to Reality", and this is shown in Fig. 3. This framework (which we will refer to as the post-Cartesian framework) states that:

- There is another entity called the Physical World which has its own objective reality independent of the Mental World. Our perception is like a window on to the Physical World and reflects a true picture of what exists in Nature.
- There is a world of mathematical objects, that is a creation of the human mind and a subset of this world serves as a model for the Physical World. A stronger version of this belief is that the Physical World at its core is nothing else but a set of mathematical equations (for an example of this see the book "Our Mathematical Universe" by Max Tegmark).
- Our brain is one of the objects that exists in the Physical World. Since other aspects of the Physical World can be modeled using mathematics, it should be possible to come up with a model for the brain which should be able to explain aspects of the mind.

The post-Cartesian framework was highly successful in discovering laws that govern the Physical World. After the the initial development of Newtonian Dynamics, other aspects of nature such Electromagnetic Waves and the Laws of Thermodynamics were discovered in the 19th century, and they all fell within this framework.
The first crack in the framework happened in the beginning years of the 20th century, with the discovery of energy quanta by Max Plack. Within a few years of that, Einstein proposed that light exhibits both wave and particle properties, and indeed within a couple of decades experiments were conducted that showed that material particles like electrons also have wavelike properties such wavelength. This raised a conundrum since for the first time the fundamental constituents of nature had a form that could not be visualized by us humans. Soon after this Heisenberg followed by Schrodinger proposed a mathematical description of microscopic phenomena that was unlike any theory that had come before. Schrodinger's version of the theory was based on a fundamental quantity called the wave function (or $\psi(x,t)$) whose nature was completely mysterious. After a lot of debate, Max Born proposed that the square of the wave function gave the probability distributon for the quantum particle being to be found in certain volume of space, which meant that the dynamics of microscopic particles  was fundamentally random. 

In parallel several interpretations of Quantum Mechanics were proposed, the most prominent being what is called the Copenhagen Interpretation. This does not try to explain the wave-particle duality, but simply states that the particle can exhibit either depending upon the experiment being performed. A quantum particle that is not being observed can be in a superposition of several states at the same time, and does not settle into a definite state until it is observed. This leads to the paradox of Schrodinger's Cat being dead and alive at the same time (this has been tested experimentally with the Aspect experiments which resulted in the 2023 Nobel Prize in Physics, not with a cat though :-)). The Copenhagen Interpretation also states that the wave function is an objective (i.e., observer independent) measure of the uncertainity that we have about the Quantum World, and this uncertainity vanishes for objects in the macro world, in other words Quantum Mechanics comes into effect for microscopic objects but is not relevant at normal scales. This last claim of the Copenhagen Interpretation has been experimentally shown to be false, so that even macro objects can exhibit wave particle duality.

The Copenhagen Interpretation is a way to shove the interpretive problems in Quantum Theory under the rug, and proceed with the day to day applications of the theory and that has worked work well in practice. However one can think of experiments in which the Copenhagen Interpretation fails, and the most well known of these is known as the problem of Wigner's Friend, which is as follows: Assume that there is a closed box containing a quantum particle that can be in one of two states, and and an Observer 1 who is located outside the box. In addition the system containing the box and Observer 1 is itself in a larger box which is closed to the outside. There is also a second observer, say Observer 2 (Wigner's friend) who is located outside the larger box. Lets say Observer 1 at some point opens the box and finds that the partcle is in State 1, at this point the Wave Function from Observer 1's point of view has 'collapsed' since there is no more uncertainity about the state of the partcle. However Observer 2 does not know this, and from his point of view, the combined system consisting of the particle and Observer 1 can be one of several possible quantum states. From this experiment it follows that there is no single observer independent interpretation of the wave function, i.e., the wave function is an a subjective measure of uncertainity in a quantum system, and cannot be separated from the observer. This is precisely the Quantum Bayesianism or Qbism interpretation of Quantum Mechanics that was proposed recently. According to Qbism the wave function is a subjective measure of quantum uncertainity and exists only in the mind of the observer, and is not really part of physical reality as such. This strikes at the heart of the post-Cartesian Framework since we can no longer claim that science is discovering Laws of Nature that exist independent of our mind. Hence the QBism interpretation of Quantum Mechanics clearly does not fall within the traditional scientific framework shown in Fig. 3, and opens the door to examining Kant's ideas as an alternative, which is discussed in the next section.
A good description of these topics can be found in Chapter 5 of the book "Putting Ourselves Back in the Equation" by George Musser.

The other problem with the post-Cartesian framework for science emerges when we try to use it to explain the mind.
The framework actually opens up the possibility of modeling the brain using mathematical models since the framework says that the brain is part of the Physical Worldand since the mind arises from the brain, it can be modeledusing mathematics. This raises the question: What is the mathematicalmodel for the mind?  How can a set of physical processes happening within the brain give rise to our mind and its subjective experiences? 

And therein lies the great mystery: 
How does the mind reflect the " objective physical reality" into the image of the world that we see in front of us?
This has no explanation within the post-Cartesian Framework, and is referred to as the "Hard Problem of Consciousness". 
In order to explain the mind, perhaps we need to go beyond the post-Cartesian Framework, just as Qbism did for Quantum Mechanics. In the next section we discuss in detail whether the Kantian Framework can solve the problems that are inherent in the post-Cartesian Framework.

## A Kantian Framework for Science

In the previous section we described how the equations of Quantum Mechanics don't describe an objective observer independent reality, but instead provide predictions that are tied to the mind of the observer doing the experiment. This appears very much like the picture of the world that Immanuel Kant drew in his philosophy. In this section we pursue this line of thinking, and also investigate how the Kantian Framework also applies to models of the mind.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant9.png) 

Figure 4

This Kantian view of Science is illustrated in Figure 4. There is no longer a split between the Physical World and the Mental World as in Figure 3, since our perceptions of reality are a creation of our minds and hence are part of the Mental World. The mathematical theories that we come up with are not describing the Noumenon, but are instead modeling the subjective perceptions created by our minds. This might also explain why mathematics is so effective in modeling Nature, this is because both mathematics and nature (as we experience it) are creations of our minds. 
The Kantian framework says that just as perceived reality is a creation of mind, at the same time the mind is one of the objects that exists in this reality. This circular path is ofen referred to as a "Strange Loop" in Cognitive Science.

An interesting aspect of this framework is that mathematical models of nature are no longer considered to be a reflection of some deep seated objective reality that exists independent of our perception. If so, what are they? Why do they seem to reflect the operation of the world?
A way to interpret these models is by using an idea called Instrumentalism, which basically says that mathematical models are a way to capture the regularities in our observations, nothing more than that. In the process of deriving the equations the scientist may hypothize some underlying mechanism, such as fields to explain electromagnetism. But these fields are a mathematical convenience and do not reflect any underlying reality. In the next section we will see that mathematical models for the mind carry this way of thinking further, such that the models are essentially a black box connecting inputs to desired outputs. For a deeper dive into this idea, please see my blog post ["Latent Variables and Latent Spaces"](https://subirvarma.github.io/GeneralCognitics/2023/12/20/LatentVariables.html).

The QBism interpretation of Quantum Mechanics clearly falls within the Kantian Framework, since both say that the practice of science, at the most fundamental level, is a purely subjective endeavour. But about theories of the mind?
There have been proposals for theories of the mind works which operate within the Kantian framework. The German scientist Herman von Helmholtz proposed one such theory in the late nineteenth century according to which the contents of perception are not given by sensory signals themselves but have to be inferred by combining these signals with the brain's expectations or beliefs about their causes.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant16.png) 

Figure 5

More recently the British neuroscientist Anil Seth has extended Helmholtz's ideas by making use what we have learnt about the brain's operation since then, which is nicely summarized in his book "Being You". His ideas are best captured by the notion of perception as a *controlled hallicunation* (see Fig. 5) and are summarized as follows:

- The brain's circuitry incorporates generic models for objects that we encounter in our lives. When the circuit is activated, the brain projects an image of the object onto our visual field.
- The brain is also constantly making predictions about the causes of its sensory signals, and these cascade in a top-down manner through the brain's perceptual hierarchies. These signals stream into the brain bottom-up (or from the outside) and keep these perceptual predictions tied  to the objects from which they originated. These signals serve as prediction errors registering the difference between what the brain expects and what it gets at every level of processing. By adjusting the top-down predictions so as to suppress the bottom-up prediction errors, the brain's perceptual best guesses maintain the connection with what is happening in the world. Thus perception happens through a continual process of prediction error minimization.
- Perceptual experience is determined by the content of the top-down predictions and not by the bottom-up sensory signals. Thus we never experience the bottom-up sensory signals themselves, but only the top-down interpretations of them.
- The brain does its predictions using the well known Baye's Rule from probability theory, with the current prediction serving as the prior, the likelihoods encode mappings from potential candidate objects that might have generated the sensory signals. The resulting posterior serves as what actually gets perceived, and also serves as the prior for the next round of predictions.
- If the perceptual priors are very strong, then it results in an actual hallicunation. On the other hand when we are paying closer attention to the external world, then the sensory signals dominate, and this results in a picture that is closer to reality.

Anil Seth's model for visual perception can be considered to be a realization of the idea that Kant originally proposed in the "Critique of Pure Reason": A way in which the chaoes of the Noumenal World are organized by the mind and projected into a picture of reality that we see round us.
In his book Seth also talks about how other aspects of the mind, such the perception of change, time, moods, emotions and even the perception of the self can also be explained by his predictive + generative model. He breaks up the hard problem of modeling the mind, into smaller problems of individual aspects of the mind, which can be tied to particular patterns of brain activity. 

Seth's work proposes mechanisms in the brain that can be used to generate aspects of the mind. Is it possible to come up with a mathematical model for the brain that is also capable of generating similar mechanisms? In the next Section we take a stab at answering this question by looking at the emerging science of Artificial Neural Networks.

## Mathematical Models for the Mind

In the previous section we described a Kantian Framework for science that proposes that since the mind is one of the objects that exists in our perceived reality, we can try to model it using the tools of mathematical modeling. The mind is special case, since we use the mind  for creating  reality, and hence in a sense the mind is trying to model itself within the confines of the perceptual boundaries that it has created. In the prior section we described Anil Seth's "Controlled Hallicunation" theory of the mind which provides mechanisms by which specific aspects of the mind emerge, and it also operates within the Kantian Framework. In this section and the in the succeeding two, we will look at how ANN models exhibit properties that are similar to that of the mind, in particular we will look at mathematical models for visual perception and language.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant10.png) 

Figure 6

Fig. 6 is a modification to Fig. 4 in which I have separated out the mind from other objects in Perceived Reality in the interests of clarity. The assumption behind this figure is that mathematical models that have proven so successful in modeling the physical world, can also be used to model the mind. The mathematical models for the former take the form of Partial Differential Equations (PDEs). But what kind of models can we use for the mind?

![](https://subirvarma.github.io/GeneralCognitics/images/Kant11.png) 

Figure 7

Fig. 7 proposes that mathematical models for the mind take the form of Artificial Neural Networks (ANNs). These models are different than Partial Differential Equations (PDEs) that are used in Physics, and point to a fundamentally different way for building mathematical models. 
Models of physical reality (on the LHS of Fig. 7) which take the form of PDEs are usually created by the minds of scientists. The process by which they are able to "guess" the right equations remains a mystery of human creativity, but some of the things they rely on includes prior work by other scientists, experimental results, mathematical theories and lastly and most importantly their own intuition.

Even though PDE models for physical reality make use of advanced mathematics, at their root they are simple, in the sense that they are able to model complex physical phenomena using only a few equations with tens of parameters. It can be argued that the reason why this is even possible is because of the *a priori* structure that our minds create in order to organize the chaos of data coming from the Noumenon. According to Kant these structures include our perceptions of space and time, and these simplify these models of quite a bit, since these imply that all interaction between entities in the model is limited to entities that are in the (space-time) neigborhood of each other. Without this simplifying assumption building mathematical models of physical reality would be impossible.
However we cannot make any such assumption in the case of models for the mind, and indeed ANNs incorporate models for individual neurons that can potentially interact with all other neurons in the model, which makes these models extremely complex. Consequently we also need billions of parameters to model all these interactions, so these models are very different than the parameter sparse PDE models. 

Unlike our mind, ANNs don't come with any simplifying *a priori* structures like space and time which have been built up over hundreds of millions of years of evolution. Before training starts, the "mind" of the ANN can be considered to be a *Tabula Rasa" or a blank slate since all their neuronal connections are set to random values (strictly speaking we do endow the ANN with an pre-existing connection topology, so it is not entirely a *Tabula Rasa*). Which is why it takes so much a data to train the ANN, in some sense we are trying to reproduce the gradual process of natural evolution within a space of a dew days of training. During this time the ANN has to discover the structure of the world through the images and text that we feed into it.

In some sense the images and text in the Training Data can be considered the be the equivalent of the Nuomenon for ANNs. Just as our mind organizes the chaos of the actual Noumenon using structures as space and time, similarly ANNs must use some organizing principle to structure the chaos of data from their Training Sets. As a result of the training, the ANN connection weights are no longer random, but assume values that enable it to recognize patterns that are present in the data.
When a pre-trained ANN is subject to new data that is not part of the Training Set, then this is analogous to sensory data coming into our brain. Due to its training the ANN is now able to extract meaning from the data and carry out tasks that are useful to us.

What are these organizing principles that an ANN uses? Are they similar to the concept of Space and Time that emerge in our minds? We will discuss this question in the following sections.

## ANN Models for Vision

Anil Seth's work provides an example of a theory that explains visual perception in our minds. His model involves a number of different mechanisms. In this section we will focus on one of them, and see how ANNs can be used to mimic the mind's ability to generate images.
This property of ANNs was discovered in the early days of Deep Learning, when ANN models were mostly used for image classification which involved inputting image pixels into a model, with the output of the model corresponding to the probability distribution over the various categories. Researchers found out that when this model was run backwards, i.e., the output was stimulated with the category corresponding to a particuler image category, then the pixels coming out at the other end actually looked like the object coresponding to that category. It didn't look exactly like the images that were fed into the ANN during the training process, but one could see that they were derived from them.
When I was first learning about Neural Networks, I remember being fascinated by this property, and it fed my subsequent interest in these systems. The fact that ANN was able to reproduce images meant that it was not merely learning enough information from the training data to classify images, but it was learning the ins and outs of what was being fed into it, sufficient for it to be able to reproduce them.

Since those very early examples of image generation, the State of the Art has progressed quite a bit, with the latest models such as DALLE-3 or Stable Diffusion being able to generate photo realistic images.

![](https://subirvarma.github.io/GeneralCognitics/images/lat51.png) 

Figure 8

The simplest way to understand how ANNs generate images, is to think of it as a mapping from the space of image pixels, to a higher level representation called a Latent Vector (which is just a bunch of numbers organized in one or two dimensions). A Latent Vector captures the information in the image, but in a highly compressed form. Images have a lot structure in them, which leads to redundancies in the way they are expressed using pixels. The ANN strips away all these redundancies and is able to capture the patterns in the image using just a few numbers, and these are captured in the form of a Latent Vector. These vectors exist in an N dimensional space called a Latent Space, and all images map to one of the vectors in this space. The Latent Space is a fantastically complex mathematical object that exists in a space with hundreds of dimensions, to which we had no access before the advent of ANNs. It has the nice property that if we interpolate between the Latent Vectors for two images, then all the vectors lying on the linear segment connecting the two also correspond to valid images (such an object is called a manifold in Differential Geometry). Hence the ANN maps image pixels to points in the image manifold, and it generates images by reversing this operation, as explained next.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant17.png) 

Figure 9

The general principle for training an ANN to generate images is shown in Fig. 9, and you may notice that it is quite similar to the Anil Seth's model for visual perception. There are two operations Encoding and Decoding.
The Encoding operation (shown in the RHS path in Fig. 9) can be likened to processing the chaotic information coming in the form of pixels, and this is done in an hierarchical fashion, which ultimately results in the creation of a Latent Vector corresponding to the image. 
The Decoding process involves turning a Latent Vector back into pixels, as is shown in the path on the LHS of Fig. 9. 
This involves taking the bare bones representation of the image in the Latent Vector form and gradually adding the shapes and textures into it until it becomes a fully formed image. Different ANN models use different architectures to implement the Encoding and Decoding pipelines, and currently the best system that we have are called Diffusion Models.
The brain is a much more complex system than ANNs, given that each neuron can have 1000s of dendrites connecting it to other neurons, and it is not fully understood how it is able to perform the encoding and decoding operations. We do have some idea of how the brain does encoding, and this was discovered in the 1960s in a series of famous experiments carried by by David Hubel and Torsten Wiesel. In fact the ANN called Convolutional Neural Network was subsequently designed to mimic what we had learnt about the brain from these experiments. The generation process in the brain is much more complex than simply inverting a Latent Vector, and involves inputs from a large number of brain regions.
The neurons in our brains do a much better job of modeling the external world, given that we are able to learn from just a few examples if new objects come into our field of vision, whereas much more data is required for training an ANN. It is quite likely that the ability to this is one of the *a priori* capabilities that has been built in our brains over 100s of millions of years of biological evolution. 

## ANN Models for Language

![](https://subirvarma.github.io/GeneralCognitics/images/Kant18.png) 

Figure 10

ANN models for language proceed along the same lines as those for vision, with the main difference being that image pixels are now replaced by individual words (there are LLMs that can be built by using individual characters instead, and those work quite well too). As shown in the top part of Fig. 10, LLMs map pieces of text to points in the language Latent Space. Just as for images, language has a lot of structure in the relationship between words, and the ANN is able to capture this structure and therby reduce the representation of the piece of text into a highly compressed form with just a few numbers. As in images, these Latent Vectors live in a high dimensional space called Latent Space, such that each point in the Latent Space corresponds to a piece of text. The discovery of this language Latent Space seems to have settled a long standing open problem in Linguistics of how language is produced. There are two aspects of langauge, namely syntax and semantics. Syntax ae the rules by which capture the grammer in language while semantics captures the meaning. There have been graphical models for syntax before the advent of ANNs, but nobody had been able to find a mathematical model that catptures the semantics. The Latent Space for language seems to be one such model, and given the complexity of the mathematical structure in which it exists, explains why it hadn't been descovered before. Interestingly enough the same ANN model, namely Transformers, has been used to build models for both images and language, which points to an underlying similarity in the structure that underlies them. We explore this further in the following section.

The bottom part of Fig. 10 shows the process by which LLMs are trained. The text from the training dataset is fed into the LLM, which then converts it into a vector in Latent Space. This vector is then decoded to regnerate the original text, and the difference between the reproduced text and the original text is used as an erro signal to train the model. In practice the text is input into the model on a word by word basis, and theoutput text is also produced likewise word by word.

![](https://subirvarma.github.io/GeneralCognitics/images/Kant19.png) 

Figure 11

Fig. 11 shows how models for language and visual perception fit within the Kantian Framework (I have left out the models for the Phenomenon or the Natural World that is shown in Fig. 7 in the interests of clarity). 



- LLMs as a Mathematical Model for Human Cognition

Wittgenstein's Philosophy of Language
- Both language and the world have structure
- Language consists of propositions, which are compounds of 'elementary propositions', which in turn are combinations of 'names. Names are the ultimate constituents of language.
- The world consists of the totality of facts, which are compounded out of 'states of affairs', which in turn are combinations of 'objects'.
- Each level of structure in the world is matched by a level of structure in language: Names denote objects, combinations of names constitute elementary propositions that correspond to state of affairs, and each of these in their turn combine to form, respectively propositions and facts.
- The arrangement of names at the most fundamental level of language structure 'mirrors' or 'pictures' the arrangement of objects at the most fundamental level of the world's structure. This called the 'picture theory of meaning' and is central to the philosophy.

![](https://subirvarma.github.io/GeneralCognitics/images/lat32.png) 

Figure 10




![](https://subirvarma.github.io/GeneralCognitics/images/lat35.png) 

Figure 11



![](https://subirvarma.github.io/GeneralCognitics/images/lat33.png) 

Figure 12






How can we tell an ANN what kind of image to generate? This is usually done by describing the image in words, but understanding how this is done requires us to understand LLMs, which is done in the next Section. In our brains images are generated in the following ways:

- The picture of the world that we see in front of us is internally generated, as Kant and Seth have told us, and uses the data coming into our eyes to modify the genration so that it corresponds to what is actually in front of us. There is no analog of this system in ANNs yet.
- When we think of some object than usually an image pops up in our "mind's eye. This image is entirely internally generated and is closer to the process by which ANNs generate iamges.
- When we are asleep and dreaming the images that we see are also entirely internally generated.







-------------------------------------------------------------------------------------------------------------------------------

## Artificial Neural Networks and LLMs

- The ANN is a model for human cognition
- Extending Kant’s thinking: The ANN can never be the same as cognition
- Mistaking ANNs for cognition can lead to antinomies
- But ANNs can approximate cognition
- Perceived Reality —> Language model of Perceived Reality —> LLM model of language 
- There is some unknown factor that makes the biological brain conscious, ANN models are not close to discovering this
- Can Integrated Information Theory ( IIT) be used as a measure of how close an ANN is to human cognition 

## LLMs within a Kantian Framework

Three latent spaces
1. latent space of perception of reality, either images or language or both together 
2. Latent space of reality is physics
3. Latent spaces of mathematics 
4. Our brain

- latent space of models of reality (physics) can be mapped on to a subset of the latent space of mathematics
- Our brain creates a mapping from perceptions to the latent space of perceptions . The points of this latent space are stored in the brain. 
- Mathematical latent  space is a creation of our brain

There are two latent spaces of reality
- Space 1 is a mapping in the brain to individual points of our perception of reality 
- Space 2 is a mathematical mapping, that is useful for doing predictions (physics). This is a mapping from our perception of reality to latent spaces of physics. 
- The mathematical space used in 2 is also a creation of the brain
- Hence physics does the following: 1) identify mathematical objects that can map to our perceptions of reality, such as fields, groups etc 2) Come up with mathematical equations that work on the mathematical objects and can be used for doing predictions (involving time and space)
- Are the regularities that we observe in nature a creation of the human brain? Then it makes sense that another human creation, ie mathematics, can be used to describe those regularities
- These regularities exist in time and space, and according to Kant, the these are a creation of the brain

Another aspect of the Kantian Framework is that Mathematical Models are always approximations and not a true picture of reality at the fundamental level. This is due to the fact these models are two levels removed from the Base Reality or the Noumenon. At the top level our minds create an abstraction of the Base Reality, and the Mathematical Models create a second level of abstraction since they are modeling the Perceived Reality which is itself an abstraction. This also means that Mathematical Models are only as good as the predictions that they make and we should not mistake the model for reality.
