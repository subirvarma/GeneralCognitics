---
layout: default
title: "From Statistical Mechanics to Neurel Networks"
---

# From Statistical Mechanics to Neural Networks



## Introduction

The steam engine was invented in the late 1700s, the inventors were brilliant tinkerers who made this advance solely through smart experimentation. But soon after, the question arose about how to make a better engine, and in particular how could one get the most work out of it. This question led to the launch of the science of thermodynamics, and the biggest early contribution was made by the great French engineer Sadi Carnot. Carnot came up with a model for an ideal engine, and showed that the efficiency of any engine is upper bounded by that of his model. This was the genesis of the second law of thermodynamics, also called the law of entropy and soon after this was joined by the first law, called the law of conservation of energy, with the efforts of Count Rumsfeld, James Joule and others. The concepts of the flow of heat and that of entropy were introduced as part of these laws, but it was a big mystery as to what exactly these were.

The first efforts in coming up with an explanation was made in the latter half of the nineteenth century, and the names associated with this are that of James Clerk Maxwell, and more importantly that of the Viennese physicist Ludwig Boltzmann. They started from the single hypothesis, that all matter is made up of tiny particles called atoms, and through a brilliant series of mathemtical investigations they were able to explain not just the true nature of heat, but also that of the mysterious entropy, and a lot more besides. In the process they founded the science of statistical mechanics, and launched a revolution in physics whose effects are being felt even to the present day. Statistical mechanics connected the microscopic properties of atoms, mainly their energy and the way they interact with each other, with macroscopis quantities that we can measure, such as temperature, specific heat etc.

Classical Newtonian machanics is a deterministic theory. Once we know the initial conditions for a system of particles, it was thought that it should be theoretically possible to predict the future behavior of the system by using Newton's Laws of motion. The conceptual leap that statistical mechanics made, was to give up on making exact predictions when the number of particles is very large and this was done by basing the new theory on the mathematics of probability theory. The theory embraced the fact that all predictions for these systems are statistical in nature, hence the results of the theory were in the form of statistical quantities such as averages and distributions. 

The biggest conceptual leap was a precise definition for the entropy of a collection of particles. Entropy had been introduced in the mid-1800s by Clausius as way of restating Carnot's results for the efficiency of the ideal steam engine, and it is something that could be measured, but what was its true nature?
Boltzmann showed that entropy was connected to our lack of knowledge of what was happening at the microscopic, and gave a formula for entropy which was entirely in terms of the probability dsitribution for the energy for the particles in the system. This did not entirely clear up the matter, since the definition implied that entropy was somehow connected to the observer, hence it was a subective quantity rather than an objective property of matter. There were paradoxes such as that of Maxwell's Demon that were thought up to illustrate this point, and this is where things stood until Claude Shannon came along in the mid part of the 20th century.

Shannon's great work in what came to be known as Information Theory, and as part of this he needed a definition for the amount of information contained in  a message, for example. The formula that he came up for doing this was precisely the same as Boltzmann's definition for entropy, even though Shannon didn't know this at that time. Shannon actually derived the formula by purely probabilistic reasoning, by looking for a measure of the amount of uncertainity in a probability distribution and this definition was applicable to any distribution whatsoever, whether it arose in statistical mechanics or information theory.
This led to a reformulation and re-thinking of statistical mechanics, in which entropy is now the primary quantity, and it was shown that the rest of the statistical mechanics could be derived from this. The only physical assumption required was an ennumeration of the microscopic particles and their energy levels.





It explained the nature of heat by postulating that heat is caused by the motion of particles, hence there was a direct relationship between the energy of particles and the temperature of a physical system.
Note that temperature is a macroscopic property, so how would one go about connecting it to microscopic energies of individual particles? This problem was solved by Boltzmann in the form of the Boltzmann Distribution, which gives the probabilties for the system to be in various energy states, under equilibrium conditions. Fundamental to deriving a formula for this distribution was the idea that the amount of disorder in a system could be precisely quantified by using the concept of entropy. Entropy had been introduced a few decades before Boltmann's work by Clausius and others in the formulation of the Second Law of Thermodynamics, but there was complete lack of understanding about it nature. Boltzmann showed that the entropy of a system is connected to our lack of knowledge of the precise details about the evolution of a system, and hence it is entirely a function of the probabilty distribution for the energy states of the system. He theorized that a system in equilibrium achieves a state of maximum entropy, i.e., our uncertainity about the state that it is in is at maximum, and this allowed him to dervive his eponymous distribution. Once the Boltzmann distribution for a system is known, macroscopic properties such average energy or entropy can be derived.

This definition of entropy introduced a new concepy: A physical quantity that is a function of both the observed system and the observer, since it is a measure of how much the observer does not know aboout the system. This was also something new to physics, and has been a source of some confusion about the nature of entropy. In the twentieth century the concept of physical quantities that depend on the observer becaome central to all of physics with the discovery of quantum mechanics. 

Statistical Mechanics became a broadly accepted theory after atomic theory amassed a huge amount of evidence early in the 20th century and stopped being a controversial subject for physicists. It also spawned a number of important advances in other fields, the first being quantum mechanics. The light inside a heated cavity can be considered to be a type of gas, but made up of photons rather than atoms. Hence it can be analyzed using the tools of Statisticl Mechanics, however the first attempts to do so did not agree with experiments. This inspired Max Planck to come up with the quantum hypothesis that energy levels in the cavity are discrete, and the rest as they is history. Another example of Statistical Mechanics concepts arising in a unrelated area, is that of Information Theory. In the 1940s Claude Shannon was looking for a measure of information contained in a message, and he hit upon a formula that was precisely the same as Boltzmann's definition of entropy (though Shannon was not aware of it as that time). Hence Shannon defined the amount of information in a message as a function of our uncertainity about the contents of the message, the more ignorant we are, greater the information, and this precisely what Boltzmann had identified as the entropy of a system.

One of the mysteries that Statistical Mechanics cleared up was that of phase transitions. This defined as the phenomenon observed when the physical properties of a collection of particles suddenly change, either as a result of variation of temperature or pressure. For example when water suddenly turns to steam at its boiling point or into ice at its freezing point. In the 1870s Van der Waals showed that phase transitions could be explained if we add a force of attraction between particles that are near each other to the model. Thus he introduced the important concept of an Interacting Particle System.
The phenonmenon of ferro magnetism is also subject to a phase transition, a magnetic material spontaneously magnetizes if its temperture is decreased below some critical point. The physicist Lenz and his student Ising came up with a model for ferro magnetism using Statistical Mechanics, and later it was shown that this model can explain the phase transition in magnetic materials.

In the last few decades Statistical Mechanics has been applied to systems in which the atomic units are people. This field has been beatifully described in the book "Critical Mass: How One Thing Leads to Another" by the science writer Phillip Ball. For example the economy can be modeled as a system whose micro dynamics are driven by individuals making decisions on things such what to buy, how much to save etc. Even racial segregation cities can be modeled as an interacting particle sysem, in which individual purchasing decisions lead from maixed neighborhood to a phase transiion that separate out the areas in which various races reside. Other interesting applications of these models have been to study allianced between countries or even between companies jockeying for an advantage in achanging industry.

In the last few decades Statistical Mechanics has been extended to non-traditional systems, and one of these is to a type of material called spin glasses. These are created in the lab by creating an alloy of a cundoctimg material such as copper with a small amount of a magnetic material. It was observed that the variation of magnetism with tempertaure for spein glasses differed significantlt from that of a pure magnet, and theories were put forward to explain this. Spein glasses by themselves haven't become commercially important, but the models that were built to explain their behavior had an unexpected side effect. They inspired the first generation of neural network models that were discoeverd in 1980s, initially by John Hopfield at Caltech, and later by Geoffrey Hinton and his collaborators. This provided a spur for additional research into neural networks which aultimately led to the models that are prevalent today.




## System with a Large Number of Interacting Particles: The 1-D Ising Model


### Energy, Entropy, Temperature



### The Boltzmann Distribution and the Partition Funtion



### Macro Properties from the Partition Function



## The Higher Dimensional Ising Model and Phase Transitions



## Spin Glass Models




## From Spin Glass to Hopfield Networks




## From Hopfield Networks to Boltzmann Machines



## From Boltzmann Machines to Modern Neural Networks




## Diffusion Models as Overloaded Hopfield Networks


Physics of IPS
- [ ] Can macro properties be derived micro behavior of particles?
- [ ] Main macro properties: Energy, Entropy, Temperature, Pressure, The nature of heat.  What is it, why does it flow from hot to cold bodies? Phase transitions.
- [ ] Heat is due to the motion of particles at the micro level. Define S,E in terms of particles, T as a function of S and E. Show that resulting system behaves as expected at macro level.
- [ ] Micro canonical system: systems with fixed E and T
- [ ] Canonical systems: systems with fixed T but varying E
- [ ] Simplest case: No interactions
- [ ] Boltzmann theory: derivation of equilibrium quantities
- [ ] Equilibrium as the solution to Entropy Maximization problem?
- [ ] Addition of interactions
- [ ] Phase transitions as a result of interactions 
- [ ] Ising models  for magnetism

From Susskind 
- [ ] Temperature, energy and entropy definition
- [ ] The Boltzmann distribution from maximum entropy principle
- [ ] The Z function
- [ ] Formula for energy, entropy etc from Boltzmann distribution 
- [ ] The Ising model, analysis using the Z function and mean field approximation 
- [ ] Equilibrium energy levels in Ising model 
- [ ] Phase transitions by varying temperature in the Ising model

Spin glasses
- [ ] When system can exist in multiple equilibra at the same time

Spin Glasses and neural networks
- [ ] Hopfield model, relation to spin glasses
- [ ] Boltzmann Machines
- [ ] Phase transitions in the Hopfield model, leading to diffusion model
- [ ] Modern neural networks. The effect of the residual connection on the energy landscape.



