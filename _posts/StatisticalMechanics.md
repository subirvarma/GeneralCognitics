---
layout: default
title: "From Statistical Mechanics to Neurel Networks"
---

# From Statistical Mechanics to Neural Networks



## Introduction

The steam engine was invented in the late 1700s, the inventors were brilliant tinkerers who made this advance solely through smart experimentation. But soon after, the question arose about how to make a better engine, and in particular how could one get the most work out of it. This question led to the launch of the science of thermodynamics, and the biggest early contribution was made by the great French engineer Sadi Carnot. Carnot came up with a model for an ideal engine, and showed that the efficiency of any engine is upper bounded by that of his model. This was the genesis of the second law of thermodynamics, also called the law of entropy and soon after this was joined by the first law, called the law of conservation of energy, with the efforts of Count Rumsfeld, James Joule and others. The concepts of the flow of heat and that of entropy were introduced as part of these laws, but it was a big mystery as to what exactly these were.

The first efforts in coming up with an explanation was made in the latter half of the nineteenth century, and the names associated with this are that of James Clerk Maxwell, and more importantly that of the Viennese physicist Ludwig Boltzmann. They started from the single hypothesis, that all matter is made up of tiny particles called atoms, and through a brilliant series of mathemtical investigations they were able to explain not just the true nature of heat, but also that of the mysterious entropy, and a lot more besides. In the process they founded the science of statistical mechanics, and launched a revolution in physics whose effects are being felt even to the present day. Statistical mechanics connected the microscopic properties of atoms, mainly their energy and the way they interact with each other, with macroscopis quantities that we can measure, such as temperature, specific heat etc.

Classical Newtonian machanics is a deterministic theory. Once we know the initial conditions for a system of particles, it was thought that it should be theoretically possible to predict the future behavior of the system by using Newton's Laws of motion. The conceptual leap that statistical mechanics made, was to give up on making exact predictions when the number of particles is very large and this was done by basing the new theory on the mathematics of probability theory. The theory embraced the fact that all predictions for these systems are statistical in nature, hence the results of the theory were in the form of statistical quantities such as averages and distributions. 

The biggest conceptual leap was a precise definition for the entropy of a collection of particles. Entropy had been introduced in the mid-1800s by Clausius as way of restating Carnot's results for the efficiency of the ideal steam engine, and it is something that could be measured, but what was its true nature?
Boltzmann showed that entropy was connected to our lack of knowledge of what was happening at the microscopic, and gave a formula for entropy which was entirely in terms of the probability dsitribution for the energy for the particles in the system. This did not entirely clear up the matter, since the definition implied that entropy was somehow connected to the observer, hence it was a subective quantity rather than an objective property of matter. There were paradoxes such as that of Maxwell's Demon that were thought up to illustrate this point, and this is where things stood until Claude Shannon came along.

In the 1940s Shannon was looking for a measure of information contained in a message, and he hit upon a formula that was precisely the same as Boltzmann's definition of entropy (though Shannon was not aware of it as that time). Hence Shannon defined the amount of information in a message as a function of our uncertainity about the contents of the message, the more ignorant we are, greater the information, and this precisely what Boltzmann had identified as the entropy of a system.
Shannon actually derived the formula by purely probabilistic reasoning, by looking for a measure of the amount of uncertainity in a probability distribution and this definition was applicable to any distribution whatsoever, whether it arose in statistical mechanics or information theory.
This led to a reformulation and re-thinking of statistical mechanics, in which entropy is now the primary quantity, and it was shown that the rest of the statistical mechanics could be derived starting from this. The only physical assumption required was an ennumeration of the microscopic particles and their energy levels, the rest of it was purely probabilistic analysis. Hene in some sense statistical mechanics can be considered to be a sub-branch of probability theory.

One of the mysteries that statistical mechanics cleared up was that of phase transitions. This is defined as the phenomenon observed when the physical properties of a collection of particles suddenly change, either as a result of variation of temperature or pressure. For example when water suddenly turns to steam at its boiling point or into ice at its freezing point. In the 1870s Van der Waals showed that phase transitions could be explained if we add a force of attraction between particles that are near each other. Thus he introduced the important concept of an interacting particle system. Needless to say phase transitions don't occur at the level of individual particles. But somehow a collection of particles collectively exhibit behavior which are not seen at the microscopic level.

Some of the biggest advances in physics in the last 100 years have been a result of applying statistical mechanics to areas which at first glance are remote its origins as a theory of gas particles. These include the following:

- The light inside a heated cavity can be considered to be a type of gas, but made up of photons rather than atoms. Hence it can be analyzed using the tools of statistical mechanics, however the first attempts to do so did not agree with experiments. This inspired Max Planck to come up with the quantum hypothesis that energy levels in the cavity are discrete, and the rest as they is history.
- Statistical maechanics can be applied to solids as well as to gases or liquids, and Albert Einstein did so in 1907 to understand the specific heat behavior with temperature for a solid. He did so by modeling it as a system of fixed particles, each of which is a simple harmonic oscillator with discrete energy levels. This model successfully predicted the experimental observations in the high temperature range. The low temperature part was corrected by Peter Debye a few years later, with his theory of phonons.
- The electromagnetic radiation within a closed chamber can be considered to be a cloud of light photons, and its physics can be analyzed using the methods of statistical mechanics. This is precisely what Max Planck did in 1901, when he noticed that in order to get agreement with experimental data, energy levels have to come in discrete chuncks, thus launching the science of quantum mechanics.
- Statistical mechanics has been used to create a model for the phenomena of electrical conduction in metals. This is done by regarding the metal as some sort of closed box that contains a cloud of electrons. The initial analysis of this model was done by < > before the sdvent of quantum mechanics, and later Sommerfield made the quantum corrections in 1927.
- in the 1920s the physicist Lenz and his student Ising came up with a model for ferro magnetism using statistical mechanics. The significance of this model is that it explained magnetism as a property of phase change in the material.

Since physicists have applied atatistical mechanics to explain the behavior of all kinds of systems, some of which are far removed from it original area of application, why not apply to systems in which the atomic units happen to be huamns? This is an active area of research, and a for a good introduction see the book
"Critical Mass: How One Thing Leads to Another" by the science writer Phillip Ball. 
For example the economy of a country can be modeled as a system whose micro dynamics are driven by individuals making decisions on things such what to buy, how much to save etc. Even racial segregation cities can be modeled as an interacting particle sysem, in which individual home purchasing decisions can often lead from a mixed neighborhood to a phase transiion that separate out the areas in which various races reside. Other interesting applications of these models have been to study alliances between countries or even between companies jockeying for an advantage in a changing industry, also as phase transitions. Sometimes a collection of humans behave differently than any one individual, this is a well established fact, leading to situations in which the mob sometimes goes out of control and does things even though the individuals making up the mob may be decent people in their everyday life. This is remniscent of the phenomenon of phase transitions in physics, in which a collection of particles exhibits new properties which are not evident at the atomic level. We will look at some models that actually show phase transitions occuring in systems made up of people, thus explaining some puzzling aspects of the collective behavior of crowds.

In the last few decades Statistical Mechanics has been extended to non-traditional systems, and one of these is to a type of material called spin glasses. These are created in the lab by creating an alloy of a cunductimg material such as copper with a small amount of a magnetic material. When the temperature was decreased, the magnetic portion tried to align itself in the same direction, but is prevented from doing so by the surrounding cunductor. This creates islands of magnetism, in which the alignment of the magnetic spins is all in different directions.
Spin glasses by themselves haven't become commercially important, but the models that were built to explain their behavior had an unexpected side effect. They inspired the first generation of neural network models that were proposed in 1980s, initially by John Hopfield at Caltech, and later by Geoffrey Hinton and his collaborators. They regarded their networks a type of spin glass, and showed that the resulting system can be made to function as an associative memory.
The neural networks that that we see today are a direct descendant of these early models and use similar principles.

## System with a Large Number of Interacting Particles

Suppose a quantity $x$ can assume the discrete values $x_i, i=1,2,..n$ with the unknown probabilities $p_i$, and all that is known is the expectation $<f(x)>$ of the function $f(x_i)$, so that

$$  < f(x) > = \sum_{i=1}^n p_i f(x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (1)                                                                                           $$          

On the basis of this information what are the best estimates of the probabilities $p_i, i=1,2,...n$? This is a classic problem in probability theory, and in order to solve it we need a measure of our ignorance of distribution. If we have such a formula that quantifies ignorance or uncertainity, then the best estimates for the $p_i$ would be those that maximize this quanitity.

But what is the formula for the amount of uncertainity represented by a discrete probability distribuion? Claude Shannon posed this question as part of his development of Information Theory, and formally showed that it is given by

$$ H(p_1,p_2,...,p_n) = -\sum_i p_i \log p_i \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2) $$               

He called this quantity the etropy of the probability distribution (note that since the $p_i < 1$, $H$ is a always positive quantity). This definition agrees with the intuitive notion that more "spread out" a distribution is, the higher is its entropy. For example if $x$ is known with certainity then $H=0$ which is its minimum value, and conversely if nothing is known about $x$, then $H=\log n$ which is its maximum value, and this is achieved when $p_i = {1\over n}, i=1,2,...n$, i.e., all of the $x_i$'s are equally possible. Unbeknownst to Shannon, this formula had been discovered a few decades earlier by Boltzmann in the context of his theory of statistical mechanics. However the formula for entropy was not central to his development of the theory which he derived using other physical considerations.
Shannon's work showed that entropy was a purely mathematical concept independent of its applications in thermodynamics. Within a few years after that, it was shown that all of statistical mechanics can be derived by taking this formula for entropy as the starting point. The only physical assumption required was an enumeration of the particles under study and their energy levels. Before we get into how this was done, lets finish the problem that was posed in the beginning of this section. For the case when the constraint on the expectation $<f(x>$ is taken into account, the maximum entropy estimate for the $p_i$'s are given by

$$ p_i = {e^{-\beta f(x_i)}\over{\sum_i e^{-\beta f(x_i)}}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3) $$                  

where $\beta$ is a constant that can be determined by substituting into equation (1) and is given by solving the equation

$$ < f(x) > = -{\partial\log Z\over\partial\beta} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4) $$

The derivation of (3) is a straightforward exercise in optimization using the method of Lagrange Multipliers in which we maximize (3) subject to the constraints (2) and $\sum_i p_i = 1$.
The denominator in this equation is a famous quantity called the Partition Function in statistical mechanics, and is denoted by $Z$ while the distribution itself is called the Boltzmann distribution. Note that the maximum entropy distribution for this case is not given by the uniform distribution $p_1={1\over n}$, and this is due to the fact that the $p_i$ values are constrained by the average $<f(x)>$ from equation (1). From the probability theory point of view, the maximum entropy estimate solved an old problem from the time of Laplace, namely what are the best probability estimates given insufficient information. Laplace recommended the use of of the uniform distribution in this situation. The maximum entropy technique allows us to improve upon by incorporatong other pieces of information, if they are available.

Up until this point, the discussion has been purely in terms of probability theory but now we are now going to use this to model a physical system. 
Consider a system that can be in one of N microstates, such that in microstate $i$ it has energy $E_i$. Also define $p_i$ as the probability that the system is in state $i$.
For example the system may be a closed box containg a gas made up of N molecules, in which case $E_i$ would be the sum of the kinetic enegies of all the molecules. Define the entropy of the system as

$$ S = -\sum_i p_i \log p_i $$

and its average energy as 

$$ E = \sum_i p_i E_i   $$

The maximum entropy principle tells us that the $p_i$ are given by the Boltzmann distribution, namely

$$ p_i = {e^{-\beta E_i}\over Z} \ \ \ where \ \ \ Z = \sum_i e^{-\beta E_i}  $$

This formula enables us to compute $E$ and $S$ as functions of $Z$, in particular

$$ E = -{\partial\log Z\over\partial\beta} $$

and

$$  S = -{\beta\over Z}{\partial Z\over \partial\beta} + \log Z $$

But we have yet to identify the significance of the constant $\beta$. Using the above formula for $S$, it follows that

$$ dS = \beta dE + E d\beta + d\log Z  $$

Since $Z$ is a function of $\beta$, this can be written as

$$ dS = \beta dE + E d\beta + {d\log Z\ d\beta\over d\beta}  $$

Using the formula for $E$ the last two terms cancel off, leaving

$$ \beta = {dS\over dE} $$

But what is the significance of the derivative ${dS\over dE}$? It can be easily shown that if two systems energies $E_1, E_2$ and entropies $S_1,S_2$ are connected to each other, and if initially ${dS_1\over dE_1} > {dS_2\over dE_2}$, then energy flows from system 1 to system 2, and in equilibrium the two derivatives are equal. This motivates the definition of temperature $T$ as

$$ {1\over T} = \beta = {\partial S\over \partial E}  $$

We are using the partial derivative since in the more general case S may be function of other variables such as the volume or an external magnetic field for example. Hence temperature enters statistical mechanics as the inverse of the Lagrange multiplier $\beta$ used to maximize the entropy! Note that we have derived some of the most important formulae in statistical mechanics by starting from the concept of entropy alone, which is quite amazing! But what is entropy? Changes in entropy can be measured in the lab, and indeed entropy was introduced into thermodynamics well before statistical mechanics came along. From Shannon-Boltzmann formula (1), it is a measure of our lack of information about the microscopic details of a system. Hence instead of worrying about the details of each individual particle. we put them in these discrete energy buckets, and this determines the entropy of the system. However this implies that the entropy is also conencted with the observer, for example an observer who has a more granular view of the system will have different measure of entropy. But how can we reconcile this with the fact that entropy can be objectively measured?
This can be explained as follows: In the equilibrium state the number of microstates that result in the same macro measurement such as energy is enormous. Hence even if we drill down to more garnular view of the system, the microstates that contribute to the average value of the macro measurement are concentrated around the same region in probability space.

There is an useful relation between the partition functions fo two or more independent systems that we will use later. Since the partition function for a single system is given by $Z = \sum_i e^{-\beta E_i}$, it follows that for two systems with partition functions  is given by $Z_1 = \sum_i e^{-\beta E_i}$ and $Z_2 = \sum_i e^{-\beta E'_i}$

$$ Z = \sum_i\sum_j  e^{-\beta (E_i + E'_j)}  $$

which can also be written as the product of the original partition functions as follows

$$ Z = \sum_i e^{-\beta E_i} \sum_j e^{-\beta E'_j}  = Z_1 Z_2 $$

If this had been the usual description of statistical mechanics, then at this point I would have introduced the model for a ideal gas, and then apply the formulae tht we just derived to compute its average energy and entropy etc. But we are going to take a slightly different path and instead analyze a system used to model ferro magnetism, called the Ising model. This system is less complex than the ideal gas, since the atoms are fixed in place rather than zippling around a box, but at the same time it enables us introduce the concept of coupling between atoms in a siimpler way than for the case of a gas. In particular coupling between atoms leads to phase transitions which can be demonstrated in the Ising model without getting into very complex analysis.

## Models for Magnetism

Certain atoms possess a magnetic moment, which we denote as $\mu$, due to the intrinsic spin of theor out-valance electrons. When an external magnetic field with intensity $H$ ia pplied, then the energy of one of these atoms is given by

$$ E = -\sigma\mu H $$

 where the spin $\sigma = +1$ if the magnetic moment of the spin aligned with the external field, and $\sigma = -1$ otherwise.  We will analyze two types of magnetic materials:

  - In paramagnetic materials, the individual spins are decoupled from one another. As a result the material only exhibits magnetic properties in the presence of an external field,
  - Ferromagnetic materials on the other hand, individual spins are coupled with those of their neighbors, as a result of which the material remans magnetized even in the absence of the external field.

The first model for magnetic materials was proposed by Lenz in the early 1920s. He gave the problem of analyzing the model to his PhD student Ising, who was able to analyze the system for the case when the atoms are arranged in $d = 1$ dimension using the tools of statistical mechanics. The case $d \ge 2$ doesn't have a simple exact solution, but it can be approximated by the mean field approximation. It was shown that models with dimension $d\ge 2$ exhibit phase transitions, which is defined a sudden change in the properties of a material when the temperature or one of the physical variables is reduced (or increased) beyond a certain threshold. Phase transitions is the most interesting property in the physics of interacting particles and the Ising model is simplest system that exhibits this behavior. As a result it has become an extremely important model, and all manners of systems have been analyzed using variations of his model. It was later found out that the mathematics of Ising models and that of quantum field theory are the same, which led to a lot of cross fertilization between the two fields.
Around 1970, the physicist Wilson revoluionized our understanding of phase transitions by introducing the theory of the renormalization group. This theory unified the physics of phase transitions occuring in a wide variety of physically disparate materials.

### Model for Paramagnetism

We will modify the notation slightly and write

$$ E = -j\sigma  $$

where $\sigma$ is the same as before and $j=\mu H$.

The partition function for a single atom at temperature $T = {1\over\beta}$, in the presence of a magnetic field can be extressed in terms of an hyperbolic function, as

$$ Z = e^{\beta j} + e^{-\beta j} = 2 \cosh(\beta j) $$

Since the atoms don't interact with each other, it follows that the partition function for a collection of N atoms is given by

$$ Z = 2^N\ \cosh^N(\beta j)  $$

so that $\log Z = N\log 2 + N\log[\cosh(\beta j)]$.  It follows that the average energy for the system is given by

$$ E = -{\partial\log Z\over\partial\beta} = - Nj\ \tanh(\beta j) $$

so that the average energy for a single atom at temperate $T$ and in the presence of the magnetic field wih coupling $j$ is

$$ {E\over N} = e = -j\ \tanh(\beta j)  $$

it follows that the average spin ${\overline\sigma}$ for a single atom is given by the hyperbolic tangent function as follows

$$ \overline\sigma = \tanh\beta j $$

![](https://subirvarma.github.io/GeneralCognitics/images/stat2.png) 

Figure 1: The hyperbolic tangent function $\tanh$

A graph of the average spin as function of the inverse temperature $\beta$ is shown in figure 1,
Since we are considering positive temperatures only, we will focus on the half plane $\beta > 0$. At very low temperatutes $\beta\rightarrow\infty$, and as a result the average spin become 1 and the average energy is minimized at $e = -1$. Hene at low temperatures each atom becomes perfectly aligned with the external magnetic field, and this is lowest energy configuration. Conversely at high temeperatures $\beta\rightarrow 0$ and as as result the average spin goes to zero, and so does the average energy. This implies that at high temperatures the system is no longer magnetized and the spins are randomly aligned in the up or down direction. As the temperature is reduce, the spins start to gradually align with the external field, but note that there is no phase change, i.e., a sudden shift from non-magnetic to magnetic.


### Ising Model in One Dimension

![](https://subirvarma.github.io/GeneralCognitics/images/stat3.png) 

Figure 2: One dimensional Ising model

The Ising model incorporates interactions between neighboring atoms and the one dimensional case is discussed in this section (see figure 2). The energy for a given configuration of spins is written as

$$  E = -j\sum_i \sigma_i\sigma _{i+1} $$

Note that unlike the previous case, there is no external magnetic field present.
Each of terms in this expression in minimized when $\sigma_i = \sigma_{i+1}$, i.e., the spins are aligned together either with $\sigma_i = \sigma_{i+1} = 1$ or $\sigma_i = \sigma_{i+1} = -1$, which implies that there are two configurations with the minimum energy value, which correspond to all the spins pointing up or all the spins pointing down. The partition function for this system is given by

$$ Z = \sum_{all\ configs} e^{-j\beta\sum_i \sigma_i\sigma _{i+1}}  $$

This sum has to be evaluated over all possible spin configurations, which makes it a non-trivial problem. The analysis can be simplified by defining a set of variable $\mu_i$ which is the product of neighboring spins, i.e.,

$$  \mu_i =  \sigma_i\sigma _{i+1},\ \ i = 1,2,...,N-1  $$

Note that $\mu$ is defines on per connection basis, rathar than on a per atom basis.
With this definition the partition function $Z'$ for any one configuration becomes

$$ Z' = \sum_{i} e^{-j\beta\sum_i \mu_i}  $$

But note that this exactly the same partition function as for the case when there is no interaction. Leveraging the solution we obatined for that case, it follows that thw partition function for a single atom is given by

$$  Z' = 2\cosh(\beta j)  $$

and that for the entire system is simply

$$ Z = 2^{N-1}\cosh^{N-1}(\beta j) $$

as a result of the independence property. It follows that the average $\mu$ value for any single connection is given by

$$ \overline\mu = \overline{\sigma_i\sigma _{i+1}} = \tanh(\beta j) $$

The correlation between the spins of neighboring atoms goes to zero as temperature increases as expected, but what about low temperatures? The above equation tells us that the average of the connection values $\overline\mu$ goes to one, but from this can we conclude that the all atoms have transitioned to the up for down spin configuration? We cannot since even if most of the spins at $\sigma_i = 1$, there can be islands of atoms with $\sigma_i = -1$, and this is consistent with having an overall average $\overline\mu$ of 1. Indeed it can be shown that the correlation between atoms separated by $n$ positions is given by

$$ \overline{\sigma_i\sigma_{i+n}} = \tanh^{n-1}(\beta j) $$

This implies that even at very low temperatures, for example for $\beta = 0.9999$, we can still make $n$ large enough so the the correlation goes to zero. This implies that there is no phase transition in the one dimensional Ising model for non-zero temperature values. It does not exhibit the phenomenon of spontaneous magnetisation in the absence of an external magnetic field.


### Ising Model for More than One Dimension: Phase Transitions

![](https://subirvarma.github.io/GeneralCognitics/images/stat8.png) 

Figure 3: The Ising model in two dimensions

Spontaneous magnetisation happens in a system when the spin state of even a single atom propagates through the material and re-orients all the spins. We just saw that in one dimension this does not happen, since the correlation between spins fades the further away we get. One way of of understanding this is by noting that a spin at a particular location has at most one spin (from the direction of propagation) which influences its own spin. However this is not the case in higher dimensions. For example for $d=2$, each atom has four neighbors, and as a result if the majority of their spins is aligned in a certain direction, then it influences the target atom to align inthe same direction. Hence he presence of multiple neighbors acts as a kind of error corerction when choosing the spin value.

Unfortunately the exact analysis of Ising models for $d\ge 2$ is extremely difficult, even for the case $d = 2$. Fortunately there exists a simple approximation method, called Mean Value Analysis, that preserves important properties such as phase transitions in the model.

The energy level at a single site is given by

$$ E = -j\sigma\sum_{i=1}^n\sigma_i  $$

where $i$ is the number of neighbors for a single atom. It is easy to see that in $d$ dimensions, the number of neighbors is given by $2d$. The Mean Field Approximation simplifies this expression by replacing ${\sum_{i=1}^n\sigma_i\over 2d}$ by its average, which we denote as $\sigma'$, so that the energy becomes

$$ E = -2dj\sigma\sigma'  $$

But this is precisely the energy level for the one dimensional Ising model with neighboring spins $\sigma$ and $\sigma'$! Leveraging the solution for this model from the prior section it follows that the average spin is given by

$$ \overline\sigma = \tanh(2dj\beta\sigma') $$

But in equilibrium the average spin at a site should be equal to the average spin that the atom sees in its neighbors, it follows that

$$ \sigma' = \tanh(2dj\beta\\sigma') $$

Making the substitution $y = 2dj\beta\sigma'$, it follows that

$$ {y\over{2dj\beta}} = \tanh y  $$

which can also be written as 

$$ {yT\over{2dj}} = \tanh y  $$

The solution $y$ to this equation corresponds to the intersection of the line ${yT\over{2dj}}$ with the function $\tanh y$, which we denote as $z_1$ and $z_2$ respectively,.

![](https://subirvarma.github.io/GeneralCognitics/images/stat4.png) 

Figure 4: $z_1 = \tanh y$ and $z_2 = {yT\over{2dj}}$ when T is high

These two functions are plotted in figure 4 for the case when the temperature $T$ is very high. In this case the line $z_2$ only intersects $z_1$ at $y=0$ which corresponds to $\sigma'=0$, i.e., all the spins are aligned in random directions. This is due to the fact that the entropy associated with random spins at high temperature values overpowers the tendency to shift to a lower energy state with all the spins aligned.

![](https://subirvarma.github.io/GeneralCognitics/images/stat5.png) 

Figure 5: $\tanh y$ and ${yT\over{2dj}}$ when $T < 2dj$

However as $T$ is reduced, then ultimately the straight line does intersect the $\tanh$ curve as shown in figure 5, and there is a critical temperature $T_c = 2dj$ at which the slope of the line is one, which is the same as the slope of of $\tanh$ at the origin. Any increase in $T$ beyond this point causes the two curves to intersect. When this happens there exists another solution $\sigma'$ which is non-zero, and this corresponds to magnetization of the material. Since there are now two solutions at average spins $0$ and $\sigma'$, the question arises which one does the system choose. 
If the system starts from a state of random spins at $T > T_c$, then it stays in this state even after the $T < T_c$, until something causes the spins to align. This is the phenomenon of phase change, and it can be triggered by the presence of an external magnetic field.
Thus the solution for $\sigma' = 0$ is unstable, and the system can tip into the $\sigma' = +1$ or $\sigma' = -1$ state very easily if $T<T_c$, as shown next.
This analysis also implies that if we start from a low temperature state and gradually increase temperature, then the magnetization abruptly switches off when the temperature crosses $T_c$.

![](https://subirvarma.github.io/GeneralCognitics/images/stat6.png) 

Figure 6: Graph of $\tanh(y + B\beta)$

In the presence of an external magnetic field with intensity $H$, the energy per atom is given by

$$ E = -2dj\beta\sigma\sigma' - H\sigma = -(2dj\beta\sigma' + B)\sigma $$

Using the same logic as before it follows that in equilibrium the average spin for the system is given by the solution to the equation

$$ \sigma' = \tanh(2dj\sigma' + H\beta)  $$

![](https://subirvarma.github.io/GeneralCognitics/images/stat7.png) 

Figure 7: Graphic solution to ${Ty\over{2dj}} = \tanh(y + H\beta)$

The solution lies at the intersection of the curves

$z_1 = \tanh(y + H\beta)$ and $z_2 = {yT\over{2dj}}$, and is plotted in figure 7. The tanh function has now shifted to the left if $B>0$, and as a result there is only solution to the equation which corresponds to $\sigma' > 0$, i.e., in the presence of the external magnetic field the solution at $\sigma'=0$ goes away. This means that if we were to start with the system in the state $\sigma'0$ and $T<T_c$, then switching on the external magnetic field insstantaneously causes the spin to become aligned with the field. This is phase change, and happens in ferromagnetic materials. Unlike for paramagnetic materials, the system statys in the magnetized state even after the external field is switched off. If the external field were pointing in the opposite direction, then it would have the system to flip to $\sigma'=-1$.



### Model for a Gas with Phase Transitions



### The Re-Normalization Group
           


## Spin Glass Models




## From Spin Glass to Hopfield Networks




## From Hopfield Networks to Boltzmann Machines



## From Boltzmann Machines to Modern Neural Networks




## Diffusion Models as Overloaded Hopfield Networks


Physics of IPS
- [ ] Can macro properties be derived micro behavior of particles?
- [ ] Main macro properties: Energy, Entropy, Temperature, Pressure, The nature of heat.  What is it, why does it flow from hot to cold bodies? Phase transitions.
- [ ] Heat is due to the motion of particles at the micro level. Define S,E in terms of particles, T as a function of S and E. Show that resulting system behaves as expected at macro level.
- [ ] Micro canonical system: systems with fixed E and T
- [ ] Canonical systems: systems with fixed T but varying E
- [ ] Simplest case: No interactions
- [ ] Boltzmann theory: derivation of equilibrium quantities
- [ ] Equilibrium as the solution to Entropy Maximization problem?
- [ ] Addition of interactions
- [ ] Phase transitions as a result of interactions 
- [ ] Ising models  for magnetism

From Susskind 
- [ ] Temperature, energy and entropy definition
- [ ] The Boltzmann distribution from maximum entropy principle
- [ ] The Z function
- [ ] Formula for energy, entropy etc from Boltzmann distribution 
- [ ] The Ising model, analysis using the Z function and mean field approximation 
- [ ] Equilibrium energy levels in Ising model 
- [ ] Phase transitions by varying temperature in the Ising model

Spin glasses
- [ ] When system can exist in multiple equilibra at the same time

Spin Glasses and neural networks
- [ ] Hopfield model, relation to spin glasses
- [ ] Boltzmann Machines
- [ ] Phase transitions in the Hopfield model, leading to diffusion model
- [ ] Modern neural networks. The effect of the residual connection on the energy landscape.



