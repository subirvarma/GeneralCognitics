---
layout: default
title: "The Path from Statistical Mechanics to AI Models"
---

# The Path from Statistical Mechanics to AI Models

## Introduction

The steam engine was invented in the late 1700s, the inventors were brilliant tinkerers who made this advance solely through smart experimentation. But soon after, the question arose about how to make a better engine, and in particular how could one get the most work out of it. This question led to the launch of the science of thermodynamics, and the biggest early contribution was made by the great French engineer Sadi Carnot. Carnot came up with a model for an ideal engine, and showed that the efficiency of any engine is upper bounded by that of his model. This was the genesis of the second law of thermodynamics, also called the law of entropy and soon after this was joined by the first law, called the law of conservation of energy, with the efforts of Count Rumsfeld, James Joule and others. The concepts of the flow of heat and that of entropy were introduced as part of these laws, but it was a big mystery as to what exactly these were.

The first efforts in coming up with an explanation was made in the latter half of the nineteenth century, and the names associated with this are that of James Clerk Maxwell, and more importantly that of the Viennese physicist Ludwig Boltzmann. They started from the single hypothesis, that all matter is made up of tiny particles called atoms, and through a brilliant series of mathematical investigations they were able to explain not just the true nature of heat, but also that of the mysterious entropy, and a lot more besides. In the process they founded the science of statistical mechanics, and launched a revolution in physics whose effects are being felt even to the present day. Statistical mechanics connected the microscopic properties of atoms, mainly their energy and the way they interact with each other, with macroscopic quantities that we can measure, such as temperature, pressure, specific heat etc. The biggest mystery that the new science was able to explain was the true nature of phase transitions, such as ice melting or a piece of iron getting magnetized. These are sudden changes in the physical properties of matter which cannot be explained without considering the joint behavior of a large number of interacting particles. 

Classical Newtonian machanics is a deterministic theory. Once we know the initial conditions for a system of particles, it was thought that it should be theoretically possible to predict the future behavior of the system by using Newton's Laws of motion. The conceptual leap that statistical mechanics made, was to give up on making exact predictions when the number of particles is very large and this was done by basing the new science on the mathematics of probability theory. The theory embraced the fact that all predictions for these systems are statistical in nature, hence the results of the theory were in the form of statistical quantities such as averages and distributions. 

The biggest conceptual leap was a precise definition for the entropy of a collection of particles. Entropy had been introduced in the mid-1800s by Clausius as way of restating Carnot's results for the efficiency of the ideal steam engine, and it is something that could be measured macroscopically, but what was its true nature?
Boltzmann showed that entropy was connected to our lack of knowledge of what was happening at the microscopic level, and gave a formula for entropy which was entirely in terms of the probability ditribution for the energy for the particles in the system. This did not entirely clear up the matter, since the definition implied that entropy was somehow connected to the observer, hence it was a subjective quantity rather than an objective property of matter. There were paradoxes such as that of Maxwell's Demon that were thought up to illustrate this point, and this is where things stood until Claude Shannon came along.

In the 1940s Shannon was looking for a measure of information contained in a message, and he hit upon a formula that was precisely the same as Boltzmann's definition of entropy (though Shannon was not aware of it as that time). Shannon defined the amount of information in a message as a function of our uncertainity about the contents of the message, the more ignorant we are, greater the information, and this precisely what Boltzmann had identified as the entropy of a system.
Shannon actually derived the formula by purely probabilistic reasoning, by looking for a measure of the amount of uncertainity in a probability distribution and this definition was applicable to any distribution whatsoever, whether it arose in statistical mechanics or information theory.
This led to a reformulation and re-thinking of statistical mechanics, in which entropy is now the primary quantity, and it was shown that the rest of the statistical mechanics could be derived starting from this. The only physical assumption required was an ennumeration of the microscopic particles and their energy levels, the rest of it was purely probabilistic analysis. Hence in some sense statistical mechanics was reduces to a sub-branch of probability theory.

One of the mysteries that statistical mechanics cleared up was that of phase transitions. This is defined as the phenomenon observed when the physical properties of a collection of particles suddenly change, either as a result of variation of temperature or pressure. For example when water suddenly turns to steam at its boiling point or into ice at its freezing point. In the 1870s Van der Waals showed that phase transitions could be explained if we add a force of attraction between particles that are near each other. Thus he introduced the important concept of an interacting particle system. Needless to say phase transitions don't occur at the level of individual particles. But somehow a collection of particles collectively exhibit behavior which are not seen at the microscopic level. The Russian physicist Lev Landau made great strides in furthering our of understanding of phase transitions in the 1940s, but a final explanation had to wait until the 1970s with the renormalization group theory proposed by the American physicist Kenneth Wilson.

Some of the biggest advances in physics in the last 100 years have been a result of applying statistical mechanics to areas which at first glance are remote fom its origins as a theory of gas particles. These include the following:

- The electromagnetic radiation within a closed chamber can be considered to be a type of gas, but made up of photons rather than atoms, and its physics can be analyzed using the methods of statistical mechanics. This is precisely what Max Planck did in 1901, when he noticed that in order to get agreement with experimental data, energy levels have to come in discrete chunks, thus launching the science of quantum mechanics. In the 1920s S.N. Bose improved upon this model for radiation, which led to Bose-Einstein statistics in quantum theory.
- Statistical maechanics can be applied to solids as well as to gases or liquids, and Albert Einstein did so in 1907 to understand the specific heat behavior with temperature for a solid. He did so by modeling it as a system of fixed particles, each of which is a simple harmonic oscillator with discrete energy levels. This model successfully predicted the experimental observations in the high temperature range. The low temperature part was corrected by Peter Debye a few years later, with his theory of phonons.
- Statistical mechanics has been used to create a model for the phenomena of electrical conduction in metals. This is done by regarding the metal as some sort of closed box that contains a cloud of electrons. The initial analysis of this model was done by < > before the advent of quantum mechanics, and later Sommerfield made the quantum corrections in 1927 using the newly proposed theory of Fermi-Dirac statistics.
- in the 1920s the physicist Lenz and his student Ising came up with a model for ferro magnetism using statistical mechanics. The significance of this model is that it explained magnetism as a result of phase change in the material. The Ising model has become the most important one in statistical mechanics since it is reasonably simple, but at the same time exhibits complex behavior.
- Phenomenona such as superconductivity and superfluidity were explained using the tools from statistical mechanics.

Since physicists have applied statistical mechanics to explain the behavior of all kinds of systems, some of which are far removed from it original area of application, why not apply to systems in which the atomic units happen to be humans? This is an active area of research, and a for a good introduction see the book
"Critical Mass: How One Thing Leads to Another" by the science writer Phillip Ball. 
For example the economy of a country can be modeled as a system whose micro dynamics are driven by individuals making decisions on things such what to buy, how much to save etc. Even racial segregation cities can be modeled as an interacting particle sysem, in which individual home purchasing decisions can often lead from a mixed neighborhood to a phase transiion that separate out the areas in which various races reside. Other interesting applications of these models have been to study alliances between countries or even between companies jockeying for an advantage in a changing industry, as phase transitions. Sometimes a collection of humans behave differently than any one individual, this is a well established fact, leading to situations in which the mob sometimes goes out of control and does things even though the individuals making up the mob may be decent people in their everyday life. This is remniscent of phase transitions, in which a collection of particles exhibits new properties which are not evident at the atomic level. We will look at some models that actually show phase transitions occuring in systems made up of people, thus explaining some puzzling aspects of the collective behavior of crowds.

In the last few decades statistical mechanics has been extended to non-traditional systems, and one of these is to a type of material called spin glasses. These are created in the lab by creating an alloy of a conducting material such as copper with small amounts of a magnetic materials such as iron. When the temperature is decreased, the magnetic material tries to align its spins in the same direction, but is prevented from doing so by the surrounding conductor. This complicates the interaction between nearby magnetic atoms which leads to some interesting properties. Spin glass models first appeared in the mid-1970s and these were extensions to the classical Ising model for ferro-magnetism. The Edwards-Anderson model was the first one that was proposed, followed by the Sherrington-Kirkpartick or SK model which proved to be more versatile and easier to analyze and has served as the most popular spin glass model since then. A full understanding of the phase transition behavior of the SK model had to wait until the mid 1980s, and the name most closely associated with this is that of the Italian physicist Giorgio Parisi. He showed that the SK model can exist in a very large number of stable states that are organized in an hierarchical tree like fashion at low temperature, and sometimes it can be in multiple states at the same time, a little bit like how water can exist in the liquid and gas phase simultaneously at its boilin point.

Spin glasses by themselves haven't become commercially important as materials, but the models that were built to explain their behavior had an unexpected side effect. They inspired the neural network models that were proposed in 1980s, initially by John Hopfield at Caltech, and later by Geoffrey Hinton and his collaborators. Hopfield regarded his network as a type of spin glass, more precisely as a modified SK model, and showed that the resulting system can be made to function as an associative memory (he was introduced to spin glasses by Phillip Anderson while both were professors at Princeton). Hinton introduced the important concept of learning the interaction rules, or weights, of these type of networks, thus resulting in the Boltzmann machine and the restricted Boltzmann machine designs. 
The neural networks that that we use today are a direct descendant of these early models and use similar principles. In fact we will see later that any type of neural network can be regarded as a spin glass model of the SK type in which the weights represent the nodes of the model while the network topology and training data govern the interaction between the nodes.

Statistical mechanics continues to be a source of inspiration for scientists seeking an explanation for the properties of neural networks, it is one of the only tools available that can analyze systems with this level of complexity. For example recently it was used to explain how diffusion models are able to create new images which are significantly different from those present in their training data.

## System with a Large Number of Interacting Particles

Suppose a quantity $x$ can assume the discrete values $x_i, i=1,2,..n$ with the unknown probabilities $p_i$, and all that is known is the expectation $f_{av}$ of the function $f(x_i)$, so that

$$  f_{av} =  \sum_i p_i f(x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (1)   $$          

On the basis of this information what are the best estimates of the probabilities $p_i, i=1,2,...n$? This is a classic problem in probability theory, and in order to solve it we need a measure of our ignorance of the probability distribution. If we have such a formula, that quantifies ignorance or uncertainity, then the best estimates for the $p_i$ would be those that maximize this quantity.

But what is the formula for the amount of uncertainity in a discrete probability distribution? Claude Shannon posed this question as part of his development of Information Theory, and formally showed that it is given by

$$ H(p_1,p_2,...,p_n) = -\sum_i p_i \log p_i \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2) $$               

He called this quantity the entropy of the probability distribution (note that since all the $p_i < 1$, $H$ is a always positive quantity). This definition agrees with the intuitive notion that more "spread out" a distribution is, the higher is its entropy. For example if $x$ is known with certainity then $H=0$ which is its minimum value, and conversely if nothing is known about $x$, then $H=\log\ n$ which is its maximum value, and this is achieved when $p_i = {1\over n}, i=1,2,...n$, i.e., all of the $x_i$'s are equally possible. 

Unbeknownst to Shannon, this formula had been discovered a few decades earlier by Boltzmann in the context of his theory of statistical mechanics. However the formula for entropy was not central to his development of the theory which he derived using other physical considerations.
Shannon's work showed that entropy was a purely mathematical concept independent of its applications in thermodynamics. Within a few years after that, it was shown that all of statistical mechanics can be derived by taking this formula for entropy as the starting point. The only physical assumption required was an enumeration of the states that the system can exist in and their energy levels. Before we get into how this was done, lets finish the problem that was posed in the beginning of this section of estimating the $p_i$ values. The maximum entropy principle tells us that that our best estimates of the $p_i$ are obtained by solving the optimization problem of maximizing $S$ subject to the constraints (1) and $\sum_i p_i = 1$. This can be done by the method of Lagrange multipliers as the maximization of $L$ given by

$$ L = -\sum_i p_i \log p_i + \alpha(\sum_i p_i - 1) +\beta(f_{av} - \sum_i p_i E_i)  $$

where $\alpha$ and $\beta$ are called Lagrange multipliers. 
This problem can be easily solved by taking the first derivatives, resulting in

$$ p_i = {e^{-\beta f(x_i)}\over{\sum_i e^{-\beta f(x_i)}}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3) $$                  

The denominator in this equation is a famous quantity in statistical mechanics called the partition function, and is denoted by $Z$ while the distribution itself is called the Boltzmann distribution.
The number $\beta$ is a constant that can be determined by substituting (3) into equation (1) and is given by solving the equation

$$ f_{av} = -{\partial\log Z\over\partial\beta} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4) $$

Note that the maximum entropy distribution for this case is not given by the uniform distribution $p_1={1\over n}$, and this is due to the fact that the $p_i$ values are constrained by the average $f_{av}$ from equation (1). From the probability theory point of view, the maximum entropy estimate solved an old problem from the time of Laplace, namely what are the best probability estimates given insufficient information. Laplace recommended the use of of the uniform distribution in this situation. The maximum entropy technique allows us to improve upon this by incorporatong other pieces of information such as the average, if they are available.

![](https://subirvarma.github.io/GeneralCognitics/images/stat13.png) 

Figure 0: A canonical system in a heat bath at temperature T

Up until this point, the discussion has been purely in terms of probability theory but now we are now going to use this to model a physical system. 
Consider a system that is in equilibrium at a fixed temperature $T$. This can be achieved by putting the system in an infinite heat bath at temperature T, and assuming that it can exchange energy, but not particles, with the heat bath (see above figure, such a system is called a canonical system in thermodynamics). The energy $E$ of the system is not fixed, but can fluctuate as shown in the right hand side of the figure. This fluctuation is due to the energy exchange with the heat bath required to manitain the temperature at a constant value T. For example if the particles were gas molecules, then the energy of each molecule is equal to its kinetic energy ${1\over 2}mv^2$. Clearly every molecule does not have the same energy and all we can do is work in terms of the distrubution of energies since we can't track the energy level of each and every molecule. Hence we are introducing some 'fuzziness' in our knowledge of the system.

Assume that the system can be in one of N microstates, such that in microstate $i$ it has energy $E_i$. Also define $p_i$ as the probability that the system is in state microstate $i$.
In the gas example, a microstate $i$ would corresponnd to the set of molecules moving around with the same energy level $E_i$.
An important example of a system is that of a ferromagnet consisting of N atoms, such that each atom is fixed in place, but can have two spin values, oriented either up or down.
in which case a microstate would correspond to a particular joint orientation of their individual spins, for example the microstate $(P,Q)$ would correspond to $P$ atoms with spins pointing up and the $Q$ atoms with spins pointing down. 
This system, which is called the Ising model, is analysed in more detail in the following sections.

Define the entropy of the system as

$$ S = -\sum_{i=1}^N p_i \log p_i\ \ \ \ \ \ \ \ (5) $$

and its average energy as 

$$ E_{av} = \sum_{i=1}^N p_i E_i\ \ \ \ \ \ \ \  (6)   $$

But what is the physical significance of this definition for entropy? Changes in entropy can be measured in the lab, and indeed entropy was introduced into thermodynamics well before statistical mechanics came along. For example for the system shown in the figure above, if its temperature is increased from $T_1$ to $T_2 > T_1$ by transferring an amount of heat equal to $Q$ from the reservoir, then the increase in its entropy is given by ${Q\over T} (called the Clausius formula)$.
But according to Shannon and Boltzmann entropy is given by formula (2), and is a measure of our lack of information about the microscopic details of a system.
This is sometimes referred to as 'blurry' view of the system, since we don't know the state of an individual particle but we can say something about how the energy is distributed among the mass of particles.
However this implies that the entropy is also connected with the observer, for example an observer who has a more granular view of the system may have different view of what a microstate is. But how can we reconcile this with the fact that changes in entropy can be objectively measured using the Clausius formula?
This can be explained as follows: In the equilibrium state the number of microstates that result in the same macro measurement, such as temperature, is enormous. Hence even if we drill down to more granular view of the system, the microstates that contribute to the average value of the macro measurement are sharply concentrated around the same region in probability space.

Going back to the example of a ferro magnetic material with with N atoms, each of which can have two spin values, a particular orientation of all the N spins together is a microstate for this system, and there are $2^N$ possible microstates. The energy $E_i$ of a microstate is a function of $(P,Q)$, where $P$ is the number of atoms with up spin and $Q = N - P$ is the number with down spin. 

The maximum entropy principle from the previous section tells us that that our best estimates of the $p_i$ are given by the Boltzmann distribution

$$ p_i = {e^{-\beta E_i}\over Z} \ \ \ where \ \ \ Z = \sum_i e^{-\beta E_i}  $$

This formula enables us to compute $E_{av}$ and $S$ as functions of $Z$, in particular

$$ E_{av} = -{\partial\log Z\over\partial\beta} $$

and

$$  S = \beta E_{av} + \log Z = -{\beta\over Z}{\partial Z\over \partial\beta} + \log Z $$

But we have yet to identify the significance of the constant $\beta$. Using the above formula for $S$, it follows that

$$ dS = \beta dE_{av} + E_{av} d\beta + d\log Z  $$

Since $Z$ is a function of $\beta$, this can be written as

$$ dS = \beta dE_{av} + E_{av} d\beta + {d\log Z\ d\beta\over d\beta}  $$

Using the formula for $E_{av}$ the last two terms cancel off, leading to the formula

$$ \beta = {dS\over dE_{av}} $$

But what is the significance of the derivative ${dS\over dE_{av}}$? It can be shown that if two canonical systems with energies $E_{av}(1), E_{av}(2)$ and entropies $S_1,S_2$ are connected to each other, and if initially ${dS_1\over dE_{av}(1)} > {dS_2\over dE_{av}(2)}$, then energy flows from system 1 to system 2, and in equilibrium the two derivatives are equal. This motivates the definition of temperature $T$ as the inverse of $\beta$, given by

$$ {1\over T} = \beta = {\partial S\over \partial E_{av}}  $$

We are using the partial derivative since in the more general case S may be function of other variables such as the volume, pressure or magnetic fields for example.

Hence temperature enters statistical mechanics as the inverse of the Lagrange multiplier $\beta$ used to maximize the entropy! Note that we have derived some of the most important formulae in statistical mechanics by starting from the concept of entropy alone, which is quite amazing! 

There is an useful relation between the partition functions for two or more independent systems that we will use later. If the two systems have partition functions given by $Z_1 = \sum_i e^{-\beta E_i}$ and $Z_2 = \sum_i e^{-\beta E'_i}$, then it follows from the independence assumption that the partition function for the joint system is given by

$$ Z = \sum_i\sum_j  e^{-\beta (E_i + E'_j)}  $$

which can also be written as the product of the original partition functions as follows

$$ Z = \sum_i e^{-\beta E_i} \sum_j e^{-\beta E'_j}  = Z_1 Z_2 $$

These are the classic formulae of statistical mechanics and have been around since the time of Boltzmann in the latter part of the 19th century. They represent a remrkable advance in our knowledge of the world, since they connect a quantity $Z$ which is a function of invisible microscopic properties of the system, with quantities such as $E_{ev}, T$ and $S$ are macroscopic quantities that we can measure with our instruments. Remember that when these furmulae were discovered atomic theory was still a controversial topic among physicists, in fact Boltzmann was at the receiving end of a lot of scorn since he based statistical mechanics on an unproven hypothesis. The discoveries of the 20th century validated his thinking, and in fact statistical mechanics served as a prototype for some of the great theories that were discovered, including quantum mechanics (by way of Planck and his theory of black body radiation) and quantum field theory. Late in the 20th century Hawking and others showed that even Black Holes possess macroscopic thermodynamic properties such as temperature and entropy.

If this had been the usual description of statistical mechanics, then at this point I would have introduced the model for an ideal gas, and then apply the formulae that we just derived to compute its average energy and entropy etc. But we are going to take a slightly different path and instead focus on the Ising model model ferro magnetism instead. This system is less complex than the ideal gas, since the atoms are fixed in place rather than zipping around in space, but at the same time it enables us introduce the concept of interaction between atoms in a simpler way than for the case of a gas. In particular coupling between atoms leads to phase transitions which can be demonstrated in the Ising model without getting into very complex analysis.

### Free Energy

There is another macro thermodynamic quantity that we will need later, and that is the Helmholtz free energy $F$, defined as

$$ F = E_{av} -  TS $$ 

This quantity is called free energy since it is the portion of system energy that can be used to do useful work, the portion $TS$ due to entropy is pure disorder which cannot be used to do work.
From the formula for entropy, it is easy to see that 

$$   F = -T\log Z   $$

From this equation you can start to see why $F$ might be an important quantity in statistical mechanics. We saw earlier that the partition function $Z$ plays a vital role as the connector between microscopic properties of a system and its macroscopic behavior. The equation above is essentially saying that $F$ and $Z$ are the same thing. It can be re-written as

$$ e^{-\beta F} = \sum_i e^{-\beta E_i} $$

This equation shows that $F$ is a macro distillation of all the microscopic energy interactions within the system. 

There is another important use of the Free Energy, which is as a way to identify the thermal equilibrium state for the system. 
From the second law of thermodynamics we know that equilibrium is characterized by the maximization of entropy, but note that this is the entropy of the system plus that of its surroundings, which is not that straightforward to characterize. 
It turns out that thermal equilibrium can also characterized as the state at which the free energy of a system is minimized (without any reference to its surroundings). 
In order to see this consider a system that starts at some temperature $T$ and also ends at the same temperature, but in the process draws an amount of heat equal to $Q$ from the heat bath. From the conservation of energy it follows that $\Delta E_{system} = Q$ Also the change in entropy for the heat bath is $\Delta S_{bath} = -{Q\over T}$. From the second law since $\Delta S_{bath} + \Delta S_{system} \ge 0$ it follows that $\Delta S_{systam} \ge {Q\over T}$. Thus

$$   \Delta F_{system} = \Delta E_{system} - T\Delta S_{system} \le Q  - T{Q\over T} = 0 $$

so that 

$$ \Delta F_{system} \le 0 $$

Hence a system that only interacts with its surroundings through the exchange of heat, the free energy never increases. Instead it decreases until it reaches a minimum when thermal equilibrium is reached. Hence in an canonical system kept at constant temperature, thermal equilibrium is the state of minimum Helmholtz free energy. This criteris is very powerful since it depends on the just the system, and is independent of the surroundings. In order to use this criteria, we need a way to calculate the free energy for a system that is not in equilibrium, we will tackle this in a later section. When scientists say that the energy of a system is minimized in equilibrium, they are referring to the free energy, since the total energy as a whole is conserved.

## Models for Magnetism

Certain atoms possess a magnetic moment, which we denote as $\mu$, due to the intrinsic spin of their outer-valance electrons. When an external magnetic field with intensity $H$ is pplied, then the energy $e$ of one of these atoms is given by

$$ e = -\sigma\mu H $$

where the spin $\sigma = +1$ if the magnetic moment of the spin aligned with the external field, and $\sigma = -1$ otherwise.  We will analyze two types of magnetic materials:

  - In paramagnetic materials, the individual spins are decoupled from one another. As a result the material only exhibits magnetic properties in the presence of an external field,
  - In ferromagnetic materials on the other hand, individual spins are coupled with those of their neighbors, as a result of which the material remans magnetized even in the absence of the external field.

The first model for magnetic materials was proposed by Lenz in the early 1920s. He gave the problem of analyzing the model to his PhD student Ising, who was able to solve the problem for the case when the atoms are arranged in $d = 1$ dimension, by using the tools of statistical mechanics. The case $d = 2$ doesn't have a simple exact solution, and it was not solved until the 1940s, and the case $d\ge 3$ is still unsolved.
However it can be shown that models with $d\ge 2$ exhibit phase transitions, which is defined as a sudden change in the properties of a material when the temperature or one of the physical variables is reduced (or increased) beyond a certain threshold. 
Phase transition is the most interesting property in thermodynamics and the Ising model is simplest system that exhibits this behavior. As a result it has become an extremely important model, and all manners of systems have been analyzed using variations of this model. It was later found out that the mathematics of Ising models and that of quantum field theory are the same, which led to a lot of cross fertilization between the two fields. In addition to Lenz and Ising, the names most associated with this model are the Russian physicists Landau and Ginzburg and Americans Wilson and Kadanoff.

### Model for Paramagnetism

We will modify the notation slightly and write

$$ e = -j\sigma  $$

for the energy of a single atom, where $\sigma$ is the same as before and $j=\mu H$ is called the coupling constant.

Hence the partition function for a single atom at temperature $T = {1\over\beta}$, in the presence of a magnetic field can be expressed in terms of an hyperbolic function, as

$$ Z = e^{\beta j} + e^{-\beta j} = 2 \cosh(\beta j) $$

Now consider a system with $N$ atoms. Since the system is paramagnetic, the atoms don't interact with each other, it follows that the partition function for a collection of N atoms is given by

$$ Z = 2^N\ \cosh^N(\beta j)  $$

so that $\log Z = N\log 2 + N\log[\cosh(\beta j)]$.  It follows that the average energy $E_{av}$ is given by

$$ E_{av} = -{\partial\log Z\over\partial\beta} = - Nj\ \tanh(\beta j) $$

so that the average energy  density $e_{av}$ at temperate $T$ (in the presence of the magnetic field with coupling $j$) is given by

$$ e_{av} = {E_{av}\over N} = -j\ \tanh(\beta j)  $$

If the average spin for the system is $\sigma_{av}$ then since $e_{av} = -j\sigma_{av}$, it follows that

$$ \sigma_{av} = \tanh(\beta j) $$

![](https://subirvarma.github.io/GeneralCognitics/images/stat2.png) 

Figure 1: Average spin $\sigma_{av}$ as a function of $\beta = {1\over T}$

A graph of the average spin $\sigma_{av}$ as function of the inverse temperature $\beta$ is shown in figure 1.
Since we are considering positive temperatures only, we will focus on the half plane $\beta > 0$. At very low temperatutes $\beta\rightarrow\infty$, and as a result the average spin become 1 and the average energy is minimized at $e_{av} = -1$. Hence at low temperatures each atom becomes perfectly aligned with the external magnetic field, and this is lowest energy configuration. Conversely at high temeperatures $\beta\rightarrow 0$ and as as result the average spin goes to zero, and so does the average energy. This implies that at high temperatures the system is no longer magnetized and the spins are randomly aligned in the up or down direction. As the temperature is reduced, the spins start to gradually align with the external field, but note that there is no phase change, i.e., a sudden shift from non-alignment to alignment, it happens gradually. On the other hand, there is a phase change if the external magnetic field is switched from $+B$ to $-B$. This causes the average spin to flip to $\sigma_{av} = -\tanh(\beta j)$ (even though the average energy remains the same). This is a sudden change in the average spin, and is referred to as a phase transition of type 1. 

### Ising Model in One Dimension

![](https://subirvarma.github.io/GeneralCognitics/images/stat3.png) 

Figure 2: One dimensional Ising model

The Ising model incorporates interactions between neighboring atoms and the one dimensional case is discussed in this section (see figure 2). The energy for a given configuration of spins is written as

$$  E = -J\sum_i \sigma_i\sigma _{i+1} $$

Note that unlike the previous case, there is no external magnetic field present.
Each of the terms in this expression in minimized when $\sigma_i = \sigma_{i+1}$, i.e., the spins are aligned together either with $\sigma_i = \sigma_{i+1} = 1$ or $\sigma_i = \sigma_{i+1} = -1$, which implies that there are two configurations with the minimum energy value, which correspond to all the spins pointing up or all the spins pointing down. The partition function for this system is given by

$$ Z = \sum_{all\ configs} e^{-j\beta\sum_i \sigma_i\sigma _{i+1}}  $$

This sum has to be evaluated over all possible spin configurations, which makes it a non-trivial problem. The analysis can be simplified by defining a set of variables $\mu_i$ which is the product of neighboring spins, i.e.,

$$  \mu_i =  \sigma_i\sigma _{i+1},\ \ i = 1,2,...,N-1  $$

Note that $\mu$ is defined on per connection basis, rathar than on a per atom basis.
With this definition the partition function $Z$ becomes

$$ Z = Z_1 + Z_2 $$

where $Z_1$ is the partition function for the case when the first spin is $+1$, and $Z_2$  for the case when the first spin is $-1$ (speification of the first spin, in combination with the sequence $\mu_i, 1 = 1,2,..,N-1$ allows us to recover all the remaining spins $\sigma_2,...\sigma_N$).
Note that both $Z_1$ and $Z_2$ are equal and are given by

$$ Z_1 = Z_2 = \sum_{i=1}^{N-1} e^{-j\beta\sum_i \mu_i}  $$

But note that this exactly the same partition function as for the case analyzed in the previous section, i.e., when there are $N-1$ atoms in a magnetic field and there is no interaction between neighboring atoms. 
Leveraging the solution we obatained for that case, it follows that

$$ Z_1 = Z_2 = 2^{N-1}\cosh^{N-1}(\beta j) $$

so that

$$ Z =  2^N\cosh^{N-1}(\beta j) $$

It follows that the average $\mu_{av}$ is given by

$$ \mu_{av} = (\sigma_i\sigma_{i+n})_{av} = \tanh(\beta j) $$

The correlation between the spins of neighboring atoms goes to zero as temperature increases as expected, but what about low temperatures. This equation tells us that the average of the connection values $\mu_{av}$ goes to one, but from this can we conclude that the all atoms have transitioned to the up for down spin configuration? We cannot since even if most of the spins at $\sigma_i = 1$, there can be islands of atoms with $\sigma_i = -1$, and this is consistent with having an overall average $\mu_{av}$ of 1. Indeed it can be shown that the correlation between atoms separated by $n$ positions is given by

$$ (\sigma_i\sigma_{i+n})_{av} = \tanh^{n-1}(\beta j) $$

This implies that even at very low temperatures, for example for $\beta = 0.9999$, we can still make $n$ large enough so the the correlation goes to zero. From this we can conclude that there is no phase transition in the 1-D Ising model for non-zero temperature values, i.e., it does not exhibit the phenomenon of spontaneous magnetisation in the absence of an external magnetic field.


### Ising Model for More than One Dimension: Phase Transitions

![](https://subirvarma.github.io/GeneralCognitics/images/stat8.png) 

Figure 3: The Ising model in two dimensions

Spontaneous magnetisation happens in a system when the spin state of even a single atom propagates through the material and re-orients all the spins. We just saw that in one dimension this does not happen, since the correlation between spins fades the further away we get, irrespective of the temperature. One way of understanding this is by noting that the spin at a particular atom has at most two other spins which directly infuence it, i.e., those of its immediate neighbors. However this is not the case in higher dimensions. For example for $d=2$, each atom has four neighbors, and as a result if the majority of their spins are aligned in a certain direction, then it influences the target atom to align in the same direction. Hence the presence of multiple neighbors acts as a kind of error corection when choosing the spin value.

Unfortunately the exact analysis of Ising models for $d\ge 2$ is extremely difficult. However there exists a simple approximation method, called mean field analysis, that preserves important properties such as phase transitions.

The energy level for a single atom is given by

$$ e = -j\sigma\sum_{i=1}^n\sigma_i  $$

where $n$ is the number of neighbors for the atom. 

The energy for a configuration of N atoms in $d$ dimensions is given by

$$  E = -J\sum_i\sum_j \sigma_i \sigma_j  $$

where the summation is done over pairs of nearest neighbors. Expressing the spins in terms of their fluctuations $\delta\sigma_i$ from their average value $m_i = (\sigma_i)_{av}$, 

$$ E = -J\sum_i\sum_j(m_i + \delta\sigma_i)(m_j + \delta\sigma_j)  $$

Expanding this expression and ignoring the product of the fluctuations $\delta\sigma_i\delta\sigma_j$ assuming it is neglegible, we get

$$ E = -J\sum_i\sum_j(m_i m_j + m_i\delta_j + m_j\delta \sigma_i)  $$

According to the mean field approximation $m_i = m_j = m$, i.e., the mean value value of the spins is the same everywhere. This leads to

$$ E = -J\sum_i\sum_j(m + m(\sigma_j - m) + m(\sigma_i - m))  $$

From translational inveriance of the atoms it follows that

$$ E = -Jm\sum_i\sum_j(m^2 + 2(\sigma_i - m))  $$

Note that $\sum_i\sum_j = {1\over 2}\sum_i\sum_{j\in nn(i)}$ where the ${1\over 2}$ factor avoids double counting pairs of sites and $nn(i)$ is the number of nearest neighbors of $i$. 
Since there is no dependence on $j$ in the summation, the inner sum is simply $\sum_{j\in nn(i)} = 2d$, where $2d$ is the number of neighbors for any atom, and $d$ is the number of dimensions. This leads to

$$ \sum_i\sum_j \rightarrow d\sum_{i=1}^N $$

The expression for energy simplifies to 

$$  E = -dJm\sum_{i=1}^N(2\sigma_i -m) $$
$$    = {NdJm^2} - 2dJm\sum_i\sigma_i $$

But this is simply the total energy level for a configuration of independent or paramagnetic atoms in the presence of a magnetic field with intensity $2dJm$.
Leveraging the solution for this model from two sections ago, it follows that the partition function is given by

$$ Z = e^{-\beta NdJm^2} 2^N\cosh^N(2dj\beta m) $$

and the average energy for the system is given by

$$ E_{av} = {\partial\log Z\over{\partial\beta}} = -2NdJm\ \tanh(2dj\beta m) $$ 

From this it follows that the average spin for the system is given by

$$ \sigma_{av} = \tanh(2dJ\beta m) $$

But in equilibrium the average spin should be equal to the mean field value, i.e., $\sigma_{av} = m$. 
Hence the following equation should be satisfied in thernal equilibrium

$$ m = \tanh(2dJ\beta m) $$

Making the substitution $y = 2dJ\beta m$, it follows that

$$ {y\over{2dj\beta}} = \tanh\ y  $$

which can also be written as 

$$ {yT\over{2dJ}} = \tanh\ y  $$

The solution $y$ to this equation corresponds to the intersection of the line $z_1 = {yT\over{2dJ}}$ with the function $z_2 = \tanh\ y$, 

![](https://subirvarma.github.io/GeneralCognitics/images/stat4.png) 

Figure 4: $z_1 = {yT\over{2dJ}}$ and $z_2 = \tanh\ y$ when $T > 2dJ$ 

These two functions are plotted in figure 4 for the case when the temperature $T$ is very high. In this case the line $z_1$ only intersects $z_2$ at $y=0$ which corresponds to $m=0$, i.e., there is no preferred orientation for the spins. This is due to the fact that the high temperature introduces thermal energy that causes some of the spins to be misaligned with the mean field.

![](https://subirvarma.github.io/GeneralCognitics/images/stat5.png) 

Figure 5: $\tanh\ y$ and ${yT\over{2dJ}}$ when $T < 2dJ$

However as $T$ is reduced, then ultimately the straight line does intersect the $\tanh$ curve as shown in figure 5, and there is a critical temperature $T_c = 2dJ$ at which the slope of the line is one, which is the same as the slope of $\tanh$ at the origin. Any decrease in $T$ beyond this point causes the two curves to intersect. When this happens there exist two other non-zero values for m, say $m'$ and $-m'$, and this corresponds to magnetization of the material. The amount of magnetization gradually increases until at very low temperatures it approaches $m = +1$ or $-1$. 

Since there are now three possible solutions at average spins $0$ and $m'$ and $-m'$, the question arises: which one does the system choose?. 
If the system starts from a state of random spins at $T > T_c$, then it stays in this state even after the $T < T_c$, until something causes the spins to align. This is the phenomenon of phase change, and it can be triggered by the presence of an external magnetic field.
Thus the solution for $m = 0$ is unstable, and the system can tip into the state $m = +1$ or $m = -1$ very easily if $T<T_c$, as shown next.
This analysis also implies that if we start from a low temperature state and gradually increase temperature, then the magnetization initially decreases and then abruptly switches off when the temperature becomes greater than $T_c$.

![](https://subirvarma.github.io/GeneralCognitics/images/stat9.png) 

Figure 6: Variation of mean field $m$ with $T$

This kind of phase transition in which there is an initial gradual decrease in the magnetic field, followed by an abrupt change to zero beyond the critical temperature, is referred to as a second order phase transition, and is illustrated in figure 6.

![](https://subirvarma.github.io/GeneralCognitics/images/stat6.png) 

Figure 7: Graph of $\tanh(y + B\beta)$

In the presence of an external magnetic field with intensity $B$, the energy for the system is given by

$$  E = -J\sum_i\sum_j \sigma_i \sigma_j - B\sum_i\sigma_i $$

The first term is due to interaction with neighboring atoms, while the second term is due to the external field.
Carrying out the same calculations as above, it can be shown that the $Z$ and $E_{av}$ are given by

$$ Z = e^{-\beta NdJm^2} 2^N\cosh^N(2dJm\beta + B\beta) $$

and 

$$ E_{av} = -2NdJm\ \tanh(2dJm\beta + B\beta) $$ 

Using the same logic as before it follows that in equilibrium the mean field for the system is given by the solution to the equation

$$ m = \tanh(2dJm\beta + B\beta)  $$

![](https://subirvarma.github.io/GeneralCognitics/images/stat7.png) 

Figure 8: Graphic solution to ${Ty\over{2dJ}} = \tanh(y + B\beta)$

The solution lies at the intersection of the curves $z_1 = {yT\over{2dJ}}$ and
$z_2 = \tanh(y + B\beta)$, and is plotted in figure 8. The $\tanh$ function has now shifted to the left if $B>0$, and as a result there is only one solution $m' > 0$ to the equation, i.e., in the presence of the external magnetic field the other two solutions away (except for the case when $\beta=0$). 
This means that if we were to start with the system in which $T < T_c$ with $B=0$, then we saw earlier there are three possible values for $m$, i.e. $m = 0$ or $m=m'$ or $m=-m'$.
However if we switch  on even a tiny amount of external magnetic field $B$, then it instantaneously causes the system to shift to $m=m'$ since the other two solutions are no langer allowed due to the shist of the $\tanh$ curve to the left i.e., the system becomes magnetized. This is a phase change, and happens in ferromagnetic materials. Unlike for paramagnetic materials, the system stays in the magnetized state even after the external field is switched off. If the external field were pointing in the opposite direction, then it would have caused the system to flip to $m=-m'$.

![](https://subirvarma.github.io/GeneralCognitics/images/stat10.png) 

Figure 9: Variation of $m$ with $T$ in the presence of an external magnetic field $B$

The variation of $m$ with $T$ for both $B>0$ and $B<0$ is shown above, and we can see that there is no phase transition. However a phase transition does occur when the field $B$ is fliped from positive to negative or vice versa, and it causes an instantaneous change in the sign of $m$. This is an example of a first order phase transition since there is a sudden change in the phase.

## The Landau Theory for Phase Transitions: Introducing the Energy Landscape

The method used to study phase transitions in the prevous section was based on the direct computation of the partition function, which in general is a tough task if the mean field approximation is not made.
There is an alternative approach to studying phase transitions, and this was discovered by Lev Landau around 1940, and is based on the computation of the Free Energy F for a system. This method significantly expanded the range of systems that could be analyzed using the methods of statistical mechanics and is now the de facto technique used. It allows us to go beyond the assumption made by mean field analysis, by allowing the field to actually vary as a function of position, thus resulting in a generalization of statistical mechanics called statistical field theory.
Free Energy based methods also serve as a starting point for ways in which statistical mechanics methods were first applied to the design of Neural Networks, as discussed in the following sections.

**The Case B = 0**

Recall that the Free Energy $F_{therm}$ for a system in thermal equilibrium at temperatutre $T$ was defined as 

$$ F_{therm} = E_{av} - TS = -T\log Z  $$

I am going to generalize the definition of Free Energy to non-equilibrium states, which is why I have added the subscript *therm* to the formula.
For a d-dimensional Ising Model, using the mean field approximation, Z was derived in the previous section for the case $B=0$, and is given by

$$ Z = e^{-\beta NdJm^2} 2^N\cosh^N(2dJm_{eq}\beta) $$

so that

$$ F_{therm} = -NdJm_{eq}^2 - {N\over\beta}\log(\cosh(2dJm_{eq}\beta))  $$

In these equations $m_{eq}$ is the equilibrium value of the mean field. Landau pointed out that this function can be defined even for the case $m$ is not the equilibrium value, thus resulting
in the free energy $F(m)$ as a function of $m$,  given by

$$ F(m) = -NdJm^2 - NT\log(\cosh(2dJm\beta))  $$

From thermodynamics we know that equilibrium occurs at the minimum value of $F(m)$, thus solving
${\partial F(m)\over{\partial m}} = 0$ leads to $m_{eq} = \tanh(2dJm_{eq}\beta)$.
which agrees with our earlier calculations. In Landau's theory, $m$ is called the *order parameter* since $m>0$ implies some degree of order (a fraction of the spins are pointing in the same direction), while if $m=0$ the spins are completely randomized.

The next step is to understand the behavior of $F(m)$ as a function of $m$. In order to do this, we first express it as a polynomial in $m$. This is facilitated by using polynomial expansions for 

$$\cosh x \approx 1 + {1\over 2}x^2 + {1\over 4!}x^4 +...\ \ \  and\ \ \ \log (1+x) \approx x - {x^2\over 2} + ...$$

Substituting these in the expression for $F(m)$ we obtain

$$ F(m) = -NT\log 2 + [NJd(1-2dJ\beta)]m^2 + ({2N\beta^3 J^4 d^4\over 3})m^4 + ...  $$

Note that $F(m)$ is symmetric with respect to $m$. Ignoring higher order terms, the derivative with respect to $m$ is given by

$$ {\partial F(m)\over{\partial m}} = 2mNJd(1-2dJ\beta) + {8m^3 N\beta^3 J^4 d^4\over 3}  $$

It follows that $F(m)$ has a single minima at $m = 0$ if $T > 2dJ$. On the other hand if $T < 2dJ$ the there are 2 minima, at

$$ m = \pm\sqrt{3(2dJ\beta - 1)\over{4(dJ\beta)^3}} $$

as well as another stationary point at $m=0$.
Remember the $T = 2dJ$ was identified as the critical temperature $T_c$ in the earlier analysis.

![](https://subirvarma.github.io/GeneralCognitics/images/stat11.png) 

Figure 10: Free Energy $F(m)$ as a function of $m$, for $T > 2dJ$ and $T < 2dJ$

$F(m)$ is plotted in figure 10, and it clearly shows the effect of varying $T$ on it shape and provides an alternative explanation of how phase changes come about.
When $T > T_c$ then there is only one stable state at $m=0$ at which the Free Energy is at a minimum, an this corresponds to the non-magnetized state.
When $T < T_c$, there are three values of $m$ at which ${\partial F(m)\over{\partial m}} = 0$, hence the system can be one of three states at equilibrium. 

The states $m = \pm\sqrt{3(2dJ\beta - 1)\over{4(dJ\beta)^3}}$ are stable corresponding to when spins are predominantly aligned in the up or down direction.
However the state $m=0$ is clearly not a stable state, since even a slight change in the value of $m$ can cause the system to transition to the other two states.
The other thing to note is that the value of the magnetization $m$ changes continuously with $T$, hence it is an example of a second order phase transition. Starting from $T>T_c$, if $T$ is gradually reduced, the two minima become gradually shallower until they disappear at $T=T_c$.

Using the equilibrium value of $m$ given by

$$ m = \sqrt{{3(T_c - T)\over{(dJ)^3\beta^2} }}  $$

we can see that $m$ has a quadratic variation with $T$ in the neighborhood of the critical temperature. This was also evident in figure 6 in the previous section. Even though this behavior was arrived at in the context of the Ising model, it turns out that all second order phase transitions for $d\ge 4$ exhibit this quadratic variation irrespective of the physical material involved. For $d = 2, 3$ the exponent is not ${1\over 2}$ from experimental data, hence the Landau theory fails for $d=2,3$. The correct exponents for these cases were computed with the help of the renormalization group theory in the 1970s.

**The Case B > 0**

The analysis for this case is exactly the same, except now the starting expression for the Free Energy is

$$ F(m) = -NdJm^2 - {N\over\beta}\log(\cosh(2dJm\beta) + B\beta)  $$

Once again, using the approximations for the $\cosh$ and $\log$ functions, it can be shown that

$$ F(m) = -NT\log 2 + NJdm^2 - {N\over{2T}}(B + 2dJm)^2  + {N\over{24T^3}}(B + 2dJm)^4 + ... $$

Note that this expression is no longer symmetric in $m$ sue to the presence of odd powers of $m$.

![](https://subirvarma.github.io/GeneralCognitics/images/stat12.png) 

Figure 11: Free Energy $F(m)$ as a function of $m$, for $B < 0$, $B = 0$ and $B > 0$

The shape of $F(m)$ as a function of $B$ is shown in figure 11, and illustrates a first order phase transition. When $B\neq 0$, $F(m)$ exhibits an asymmetric shape as function of $m$, such that for $B>0$ the minima that occurs for $m>0$ is deeper than that for $m<0$ (and vice versa if $B<0$). The shallower minima corresponds to a meta-stable state, and the system transitions to the more stable deeper minima by traversing the energy barrier between the two. If the sign of $B$ is flipped, then it causes an instantaneous change in the magnetization $m$ which also changes sign, and this is characteristic of first order phase transitions.           


## Spin Glass Models: Frozen Complexity

![](https://subirvarma.github.io/GeneralCognitics/images/stat14.png) 

Figure 12: Spin Orientation in Ferromagetic, Anti-Ferromagnetic and Spin Glass materials

Back in the 1950s scientists were actively investigating the properties of new materials created as a result of mixing two or more elements. This was a fruitful avenue of research and resulted in the discovery of semiconductors, that were created by adding impurities such as phosphorus to silicon. In the same spirit a group at Bell Labs created a new material by adding ferromagnetic atoms, such as iron, to a conducting substrate, such as copper. When they studied the magnetic properties of this material as a function of temperature, they found something interesting. 

- When the temperature exceeded some critical point, there was no magnetism detected, which is just as in ferromagnetic materials
- Below the critical temperature, the material become magnetic. However, after gradually increasing as the temperature was further reduced, the magnetisation hit a limit at a finite temperature and stayed there even at lower temperatures.

This was a new kind of magnetic behavior not seen before, and soon physicists came up with an explanation for it. As shown in figure 12, all the spins of ferromagnetic materials tend to align at lower temperatures, while those in anti-ferromagnetic also tend to align but in opposite directions. Spin Glasses on the other hand do not exhibit any such regularity. Even at low temperatures, their spins can have different orientations as shown in the right hand side of the figure, and once a particular orientation is reached, it remains frozen as the temperature is further reduced towards zero. Hence some of the interactions are ferromagnetic (shown in blue), while others are anti-ferromagnetic (shown in red).
It seems that the presence of the copper atoms in the spin glass interfere with the tendency of iron atoms to try to line up as temperature decreases. This random orientations of frozen spins is said to be "dis-ordered". In the same way that an amorphous solid like window glass doesn’t have an orderly crystal structure, a spin glass doesn’t have an orderly magnetic structure. But is there any structural law that these seeemingly random frozen spin configurations obey? This turned out to be a very difficult theoretical problem, and the solution did not emerge for another three decades.

![](https://subirvarma.github.io/GeneralCognitics/images/stat15.png) 

Figure 13: Ordering of Spins

Systems can be arranged in a continuum as shown in figure 13, with systems exhibiting complete order (such as a crystal) on the left hand side, and systems that exist in a completely disordered state (such as a gas) on the right hand side. These two extremes are relatively easy to model, as we have seen in our sections on statistical mechanics. However since spin glasses exist in the in-between space of partial disorder, whose structure does not repeat, they are particularly difficult to analyze. Many systems, especially those that fall outside the realm of physics, such as the behavior of crowds or road traffic, fall into this in-between category. Hence if the spin glass system can be solved, then it can lead to greater understanding in multiple other areas.

![](https://subirvarma.github.io/GeneralCognitics/images/stat16.png) 

Figure 14: Energy Landscape in Spin Glasses

Something that was realized pretty early in the study of spin glasses is that below the critical temperature, their energy landscape is quite unlike that for magnetic ferromagnetic materials. It consists of multipe peaks and valley as shown in figure 14.

### Spin Glass Models: Edwards Anderson (EA) and Sherrington Kirkpatrick (SK) Models

![](https://subirvarma.github.io/GeneralCognitics/images/stat17.png) 

Figure 15: Spin Interactions in the Sherrington Kirkpatrick Model

Samuel Edwards and Phillip Anderson were the first physicists to come up with a mathematical model for spin glasses. In the proces they introduced several new theoretical ideas that have proven to be very useful, but their model itself was a bit difficult to analyze. Shortly therafter, David Sherrington and Scott Kirkpatrick introduced their epynomous model (which we will call the SK model), which was a simplified version of the EA model, and this proved to be very influential in the subsequent years, since the model was easier to analyze also captured some of the essential aspects of spin glasses.

Recall that the energy function for the Ising model was given by

$$ H = -J\sum_i\sum_{j[i]}\sigma_i\sigma_j $$

This assumed that all interactions were confined to neighboring atoms ($j[i]$ being the neighbors of $i$), and more importantly the strength of the interaction $J$ is the same for all interactions. Sherrington and Kirkpatrick made the following modifications to this:

- Each atom can interact with all the other atoms in the lattice, and moreover interactions always happen in a pairwise fashion (see figure 15). This is called the Fully Connected assumption and differentiated the SK model from the earlier EA model that used neighboring interactions only. This assumption simplified the mathematical analysis of the model.
- As before, the spins $\sigma_i$ can assume values $\pm 1$.
- The strength of the interaction $J_{ij}$ is a function of the two atoms taking part in the interaction and can vary randomly in magnitude as well as sign.

The second assumption is an important one, since it captures the fact that the random spin orientations and variable distances between atoms lead to variable coupling strengths.
Under these assumptions the energy function for $N$ atoms becomes

$$ H_N = -\sum_i\sum_{j[i]} J_{ij}\sigma_i\sigma_j $$

But what is the nature of the spin interactions $J_{ij}$? Sherrington and Kirkpatrick assumed that $J_{ij}$ can be written as

$$ J_{ij} = {J\over\sqrt{N}}  $$

where $J$ is a random variable that obeys the Standard Normal distribution, i.e.,

$$  J  \sim {1\over{\sqrt{2\pi}}} e^{-{x^2\over 2}} $$

The scaling $\sqrt{N}$ ensures that the total energy does not blow up to infinity as $N$ increases, while the Standard Normal assumption completely randomizes all interactions. Note that for a particular realization of the random variable $J$, the interactions are fixed, or quenched, but at random values. This observation lies at the heart of the SK model, i.e., the interactions can assume random values, but they follow a well defined statistical distribution.
Because of the quenched disorder, every energy function $E$ in a spin glass is different from all others, since every sample corresponds to a different set of coupling $J_{ij}$. Even though samples are microscopically different, the display the same macroscopic behavior on the average.

Recall that for the case of ferrmagnetism, the magnetized phase was characterized by the fact that the average magnetization $m$ was non-zero. If $E(\sigma_i) = m_i$, then

$$ {1\over N}\sum_i m_i = m > 0 $$

In the SK model clearly ${1\over N}\sum_i m_i = 0$ once we average over all possible values of the the interaction $J$, which has an average of zero. So how can we characterize the magnetized phase in SK models?
If a low temperature phase exists for the SK model, there must some spin configurations that are more likely to occur than others.
Edwards and Anderson suggested that we use the following critera instead

$$ q_{EA} = {1\over N}\sum_i m_i^2 > 0  $$

where $q_{EA}$ is called the Edwards-Anderson order parameter.

So how do we go about analyzing this model? Using the Landau theory for phase transitions, we start with the free energy density function $f_N(J)$ for $N$ atoms and for a particular realization of the interaction stregth $J$, given by

$$ f_N(J) = -{1\over{\beta N}} \log{\sum_{\sigma}e^{-\beta H_N}} = -{1\over{\beta N}}\log Z  $$

where the summation is over all possible configurations of the spins.
The free energy density ${\overline f}(N)$ for $N$ particles is then obtained by averaging over the coupling distribution $J$ 

$$ {\overline f(N)} = \sum_{J} P[J] f_N(J) = -{1\over N\beta}E(\log Z) $$

and then taking the limit as $N\uparrow\infty$, we  finally obtain

$$ {\overline f} = \lim_{N\uparrow\infty}{\overline f(N)} $$

This is an exteremly difficult problem in probability theory. In the 1960s a way to solve it arose in the context of Quantum Field Theory, and is known as the replica method. It works as follows:
With the random interactions frozen at $J_{ij}$, consider $n$ independent replicas of the system. Replicas have the same couplings $_J_{ij}$, but evolve independently, so that they can end up with a different spin configuration.
Using the replica idea, the overlap function $q_{ab}$ between two replicas is given by

$$ q_{ab} = {1\over N} \sum_{i=1}^N E(s_i^a.s_i^b) $$

Define a partition function $(Z_J)^n$ for the system of $n$ replicas by

$$ (Z_J)^n = \sum_{(s)^1}\sum_{(s)^2}...\sum_{(s)^n}e^{-\sum_{a=1}^n \beta H_J[s^a]} $$

where each of the summations $\sum_{(s)^i}$ is over all possible configuations of the spins in a replica that are compatible with the frozen interaction $J$.
The average over the distribution of $J$ is given by

$$  E(Z^n) = \sum_J p(J) (Z_J)^n $$

At the end of this step we still have $n$ replicas, but they are no longer independent. Indeed they are correlated due to the fact that their spins are constrained by the fact that they all follow the common interaction law given by $J$.
The free energy density for the system of replicas is defined by

$$ f_n(N) = -{1\over{\beta Nn}}\log E(Z^n(N)) $$

Since $E(\log Z) = \lim_{n\rightarrow 0} {E(Z^n) - 1\over n}$, it follows that

$$ \lim_{n\rightarrow 0} f_n(N) = -{1\over{\beta N}}E(\log Z(N)) = {\overline f(N)} $$

Taking the limit assumes that $E(Z^n(N))$ contunes to be well defined even when the number of replicas $n$ is not an integer, which is known as analytical continuation in mathematics.
Finally taking the limit $N\uparrow\infty$, the free energy density is given by

$$ {\overline f} = \lim_{N\uparrow\infty} {\overline f(N)} $$

Note that by using the replica trick, we have replaced the calculation of $E(\log Z)$ by $\log E(Z)$, which helps in simplifying the computation.

It can be shown that $E(Z^n(N))$ can be written as a function of the overlap matrix $Q_{ab}$ which describes the overlap between two replicas $a$ and $b$:

$$ E(Z^n(N)) = \int \prod_{(ab)} {dQ_{ab}\over{2\pi}} e^{NA[Q_{ab}]}  $$

Taking the limit $N\rightarrow\infty$, this expression can be evaluated by the saddle point approximation method, which leads to

$$ f_n = -\lim_{N\rightarrow\infty} {1\over{\beta Nn}}\log E(Z^n(N)) = {1\over{\beta n}} A_{sp}[Q_{ab}] $$

where $A_{sp}$ is evaluated at the value of $Q_{ab}$  which achieves its saddle point, i.e., ${\partial A\over{\partial Q_{ab}}} = 0$. The free energy ${\overline f}$ is then evaluated by taking the limit of $f_n$ as $n\rightarrow 0$. 

So how do we get hold of the values of $Q_{ab}$ at the saddle point? The way scientists have proceeded is by making educated guesses (called *ansatz*), and then verifying that the math works out. The simplest structure for $Q_{ab}$ is known as the replica symmetric (RS) solution. In this case the overlap $q_{ab}$ between any two replicas is the same, i.e., $q_{ab} = q_0$ for $a\ne b$ and $q_{aa} = q_d$ for self overlap along the diagonal. This ansatz correctly describes the high temperature regime, but it becomes unstable as the temperature is lowered below a threshold $T_c$ and this correponds to a phase change.




 


![](https://subirvarma.github.io/GeneralCognitics/images/stat18.png) 

Figure 16: Variation of the Edwards Anderson Order Parameter with temperature







### The Parisi Solution to SK Model: Complex Energy Landscapes




## From Spin Glass to Hopfield Networks: Engineering the Energy Landsacpe




## From Hopfield Networks to Boltzmann Machines: Restricted Boltzmann Machines, Deep Boltzmann Machines






## Modern Neural Networks as Modified SK Models



### Diffusion Models as Overloaded Hopfield Networks





Physics of IPS
- [ ] Can macro properties be derived micro behavior of particles?
- [ ] Main macro properties: Energy, Entropy, Temperature, Pressure, The nature of heat.  What is it, why does it flow from hot to cold bodies? Phase transitions.
- [ ] Heat is due to the motion of particles at the micro level. Define S,E in terms of particles, T as a function of S and E. Show that resulting system behaves as expected at macro level.
- [ ] Micro canonical system: systems with fixed E and T
- [ ] Canonical systems: systems with fixed T but varying E
- [ ] Simplest case: No interactions
- [ ] Boltzmann theory: derivation of equilibrium quantities
- [ ] Equilibrium as the solution to Entropy Maximization problem?
- [ ] Addition of interactions
- [ ] Phase transitions as a result of interactions 
- [ ] Ising models  for magnetism

From Susskind 
- [ ] Temperature, energy and entropy definition
- [ ] The Boltzmann distribution from maximum entropy principle
- [ ] The Z function
- [ ] Formula for energy, entropy etc from Boltzmann distribution 
- [ ] The Ising model, analysis using the Z function and mean field approximation 
- [ ] Equilibrium energy levels in Ising model 
- [ ] Phase transitions by varying temperature in the Ising model

Spin glasses
- [ ] When system can exist in multiple equilibra at the same time

Spin Glasses and neural networks
- [ ] Hopfield model, relation to spin glasses
- [ ] Boltzmann Machines
- [ ] Phase transitions in the Hopfield model, leading to diffusion model
- [ ] Modern neural networks. The effect of the residual connection on the energy landscape.



